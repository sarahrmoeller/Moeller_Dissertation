\chapter{Automated Segmentation and Glossing for Documentary and Descriptive Linguistics}
\label{chap:seggloss}

This chapter examines how variations in research design could affect the integration of machine learning for morpheme segmentation and glossing of under-documented languages. Morpheme segmentation and glossing are traditionally the first tasks undertaken in the documentary and descriptive workflow after transcription. Both segmentation and glossing provide essential linguistic information from which deeper analysis can be done. Segmenting words into morphemes can reduce confusion in NLP models that is caused by data sparsity because it clarifies relationships between word forms. Glosses make implicit morphosyntactic structures explicit and accessible for analysis. Segmented and glossed text can be leveraged to improve NLP systems in low-resource settings, such as for machine translation \citep{shearing_improving_2018,zhou_using_2020}. Therefore, automating these tasks and integrating that automation as technological assistance for documentary and descriptive linguists would benefit both linguistics and NLP.

Whenever two disciplinary fields are brought together for mutual benefit, different expectations or accepted conventions will also meet. These expectations may seem to clash. This chapter addresses potential clashes that stem from differing approaches to the same tasks in natural language processing (NLP) and linguistic analysis. The approaches are based on, or have led to differing conventional expectations about methods or data. For example, it is generally expected in NLP the textual data that is processed will be an orthographic representation, whereas linguists may prefer to work with a  morphophonological representation with the goal of processing underlying linguistic forms. Such differences can make interdisciplinary collaboration unnecessarily complicated or perplexing. 

When these differences affect overall research design, it is easy to simply choose one or the other conventional approach without testing which choice might actually benefit the task at hand or be more efficient for long-term goals. This chapter compares the short-term affect of three pairs of differing expectations which have arisen during the authors' research. 
The first study examines \textit{how a choice of morpheme segmentation strategies affect NLP performance}. Linguistic theory assumes the existence of underlying, or canonical, morphemes and the segmentation strategy choice is guided by the goal to discover those forms. Canonical segmentation represent morphemes in their theoretical, underlying forms, which allows orthographic changes triggered by surrounding phones to be ignored. This contrasts with surface segmentation which simply inserts segment breaks in the orthographic representation, thereby indicating surface segments or ``morphs'' \citep{virpioja_empirical_2011}. Since NLP almost always deals with orthographic representations, its systems are trained to perform surface segmentation almost exclusively. 
It might seem reasonable that linguists who want to integrate automated assistance should adjust their strategy to match NLP expectations. But without testing, are we sure that NLP systems perform better at one strategy over the other?

The second study asks \textit{whether morpheme segmentation and glossing should be approached jointly or sequentially}. In other words, is an NLP system trained to do segmentation and glossing simultaneously as a joint task better than a system trained to treat them as two separate tasks? Instead of arbitrarily choosing one or the other method, we should test whether one approach achieves more accurate results on its task. If the sequential approach is more accurate, then linguists might want to consider adjusting their workflow in order to gain optimal benefit from NLP integration, but if the joint task approach performs better, then perhaps NLP would benefit by adjusting their approach to match how the linguists' produce new language data.

The third pair of differing expectations is not between linguistics and NLP but within NLP. This study looks at \textit{whether a state-of-the-art deep learning model can outperform feature-based models.} Until recently, feature-based models regularly outperformed deep learning models in low-resource settings. For example, previous work on Lezgi \citep{moeller_automatic_2018} used the same corpus as the current work does and found the CRF outperformed the then state-of-the-art LSTM.\footnote{A direct comparison cannot be made to this work because in that work only affixes were glossed while the current work also glosses roots.} Yet, as deep learning models improve, it is becoming less certain which type of model is best in low-resource settings so it is still important to compare the two types. Linguists may want to know what models to recommend when they begin collaboration with computer scientists in NLP. Additionally, if feature-based models are consistently more accurate, then both linguists and NLP scientists will want to know what features can be extracted for good results across languages. 

Only five of the corpora presented in \autoref{tab:dissdata} could be used for all experiments because they were annotated in FLEx and so have both surface and and canonical morpheme segments: Alas, Lamkang, Lezgi, Manipuri, Nat\"ugu, Upper Tanana. Their statistics of their corpora are shown again in \autoref{tab:segglossdata}. Results from other languages are included in the tables where available but only results from these five languages are considered in the analysis. 

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|r|rc}
         \textbf{Language} & \textbf{Tokens} & \multicolumn{2}{c}{\textbf{Seg/Gloss}} \\
         \hline
         Alas & 4.5k & 3,775 & 85\%  \\
         \hline
         Lamkang & 101k & 49,699 & 49\% \\
         \hline
         Lezgi & 14k & 13,353  & 94\% \\
         \hline
         Manipuri & 12k & 11,907 & 98\% \\
         \hline
         Natügu & 16.5k & 12,435 & 75\%  \\
         \hline
         Upper Tanana & 17.5k & 11,867 & 67\% 
    \end{tabular}
    \caption[Data for Segmentation and Glossing Experimentation]{The percentage and total number of tokens in the corpora that are both segmented (canonical and surface) and glossed.}
    \label{tab:segglossdata}
\end{table}

The rest of this chapter describes the experiments that test segmentation and glossing systems and compare the results between a surface and canonical strategy, between a joint and sequential approach, and between feature-based and deep learning models. The experiments are described in \autoref{sec:sgmethodology}. Their results are presented in \autoref{sec:sgresults}. The deep learning results are analyzed and discussed in \autoref{sec:sgDLanalysis} and the feature-based models are discussed in \autoref{sec:sgFtrAnalysis}. 


\section{Experiments}
\label{sec:sgmethodology}

All tasks are treated as a problem of converting an input sequence of characters $\vect{x} = (x_1, \ldots, x_n)$ to an output sequence of labels $\vect{y} = (y_1, \ldots, y_n)$. The output sequence of labels indicate the (canonical or surface) morpheme and/or the morpheme's gloss. Pilot work showed that when the context of the whole sentence was provided during training performance decreased, so each input data instance is a word (or in the case of the SVM, a morpheme). 

It was assumed that the documentary and descriptive field data had only been segmented and glossed. No other information was leveraged from the IGT or other resources. Gold standard data was created by filtering out tokens that were not completely segmented (both canonical and surface) or glossed. This was determined by assuring that the surface, canonical, and gloss lines aligned with each other. 
Glosses were standardized by capitalizing affix glosses. When morphemes had multiple English words or symbols in their glosses, the words were joined by periods.
Morpheme boundary markers such as hyphens ( - ) and equal signs ( = ) were preserved to distinguish clitics from bound morphemes and to indicate relative ordering of morphemes (i.e. pre-/suf/infixing); angle brackets ( \textlangle{}\textrangle{} ) were used to denote circumfixes. 

The data was arranged to accommodate both joint and sequential approaches to the tasks. This was done after withholding ten percent of the corpus as a test set, by dividing the remaining data into two equal training sets. Ten percent of each half was used as a development set. For easier comparison, the joint models were trained on only one half -- the same part that was used for the segmentation-only step in the sequential approach. For each experiment with the Transformer model, a ten-fold cross validation was run but since the CRF took significantly longer to train, the feature-based models were run only once.


\subsection{Surface vs. Canonical Segmentation Strategies}
\label{sec:sgstrategies}

This experiment compares the Transformer's performance when trained on different segmentation strategies. Canonical segmentation gives more information about a language's underlying morphological structure, but at the same time, it reduces the number of unique labels and reflects allomorphy and morphophonological processes that might not be represented in the orthography. On the other hand, surface segmentation does not require the computational models to learn allomorphy or morphophonology \citep{goldsmith_computational_2017} but also does not provide a thorough analysis of the language's morphology. It simply divide the strings of surface text into surface segments known as ``morphs'' without regard to potential relationships between them. 

The two segmentation strategies are compared in (\ref{ex:canseg}) where the first two surface letters of each word in (\ref{ex:canseg1}) are represented by identical canonical segments in (\ref{ex:canseg2}). In practice, both strategies are encountered during language documentation and description, the initial strategy depending in part on software tools. For example, the older, but still popular, Toolbox\footnote{https://software.sil.org/toolbox/} allows surface segmentation whereas ELAN \citep{auer_elan_2010} supports both but as separate tasks, while FLEx \citep{baines_overview_2018} requires surface segmentation but facilitates simultaneous canonical segmentation.\footnote{The Arapaho and Southern Sierra Miwok corpora could not be used for this experiment because they were annotated in Toolbox.}  


\pex   
\label{ex:canseg}
\a il-legal \hspace{6mm} in-capable \hspace{5mm} im-mature
\label{ex:canseg1}
\a in-legal \hspace{5mm} in-capable \hspace{5mm} in-mature
\label{ex:canseg2}
\a \textsc{neg}-legal \hspace{1mm} \textsc{neg}-capable \hspace{1mm} \textsc{neg}-mature
\label{ex:canseg3}
\xe

%This seems more natural since linguists are segmenting based on their understanding of the morphemes function/meaning, so in most cases a morpheme is segmented only because its gloss has been identified. So an automated system that assumes the steps occur consequently instead of simultaneously/jointly may not work well for linguists. It may be ever be accepted or may be difficult to integrate. It may make it difficult for linguists to benefit from automated assistance. On the other hand, this assumption of separate segmentation and glossing may point linguists to a better way. Linguists may benefit by integrating this NLP assumption into their work. 

The intention of this study is not to provide a direct comparison between models trained on the two strategies because technically the surface and canonically segmented data are different datasets. The study assumes that if one strategy was conducted first, then the other type of segmentation might be more easily learned from it. Therefore, if one strategy is consistently results in a more accurate model, that strategy should perhaps be adopted as an earlier step in the documentary and descriptive workflow. In other words, if a corpus could be surface segmented with very high accuracy based on an initial hypotheses of morpheme segments in the transcription, then first predicting surface segments for the whole corpus might make it easier and faster to later discover the canonical, underlying morphemes, as well as matching a conventional expectation in NLP. 

The difference between the two machine learning models is their outputs. The input does not change; it is the same as the models described in \autoref{sec:sgjoint}. An English example of the output for surface segmentation is shown in (\ref{ex:surfout}) and the corresponding output for canonical segmentation is in (\ref{ex:CanOut}).

\pex   
\label{ex:CanInOut}
\a<a> \textbf{SURFACE:} \hspace{2 mm} tax\#levy \hspace{3 mm} -es\#\textsc{pl}
\label{ex:surfout}
\a<b> \textbf{CANON.:} \hspace{4 mm} tax\#levy \hspace{3 mm} -s\#\textsc{pl} 
\label{ex:CanOut}
\xe

For this study, only the corpora that had been interlinearized with FLEx was selected because FLEx allows the annotators to provide both surface and canonical segments but the other methods used only one or the other strategy. 
Even though those five projects employed the same software tool, the corpora still had differences in formatting. For example, in FLEx only one part of the circumfix is labeled as circumfix, the other part as prefix or affix. Both Natugu and Alas were annotated for circumfixing, but in one corpus the prefixed part was labeled as a circumfix and the suffixed part as a suffix, while in the other corpus the opposite approach was taken. Such variations had to be identified and re-formatted.

The surface morphs and underlying morphemes datasets had to be handled slightly differently for the two strategies. The most obvious difference is the handling of circumfixes. Surface representation preserves the ordering of morphs and does not require knowledge of morpheme types, so the two parts of circumfixes were treated as two different prefix and suffix morphs.  On the other hand, canonical segmentation represents the circumfixes as a single morpheme that repeats before and after the stem. The different ways of handling the two strategies are shown in (\ref{ex:Circumfixes}).

\pex   
\label{ex:Circumfixes}
\a<a> \textbf{SURFACE:} \hspace{2 mm} ke- \hspace{4 mm}  STEM  \hspace{1 mm} -en
\label{ex:circumsurf}
\a<b> \textbf{CANON.:} \hspace{1 mm} ke\textlangle{}\textrangle{}en- \hspace{1 mm} STEM \hspace{1 mm} -ke\textlangle{}\textrangle{}en
\label{ex:circumcan}
\xe

%Null morphemes should have been handled differently, but they weren't! They should have been eliminated it was discovered they remained because the selection of labels was based on the gloss line.


\subsection{Joint vs. Sequential Segmentation and Glossing}
\label{sec:sgjoint}

Joint versus sequential approaches to segmentation and glossing were tested and compared to see whether joint or sequential segmentation and glossing is a more accurate approach to interlinearization when integrating automated assistance. Joint segmentation assumes that segmented data without glosses is unlikely to be commonly available because when linguists identify a morpheme it usually means they have already determined the morpheme's meaning.\footnote{Field linguist Lindy Pate (p.c.) believed that poorly educated native speakers in Papua New Guinea could segment morphemes without knowing the glosses. In my experience with Lezgi, this is true. It may result in more frequent segmentation errors but it certainly seems easier for non-linguist native speakers to segment than to gloss.} 
Joint segmentation requires the model to learn the morpheme boundary and gloss simultaneously. The sequential approach presupposes that glossing happens after the whole text is segmented. It assumes that segmentation is easier or faster to do than joint segmentation and glossing or that unsupervised segmentation tools such as Morfessor \citep{smit-etal-2014-morfessor} are reasonably accurate.

The models of joint learning take an input that is a character-level representation of a word, as shown in (\ref{ex:Jointin}). Each character is treated as as separate symbol by the model. The output is a sequence of labels, one label per morpheme, as shown in (\ref{ex:JointOut}). The label combines the morpheme's shape and gloss, separted by a hashtag or pound symbol; this symbol is chosen because it did not appear in the first eight corpora.\footnote{It was used infrequently in Upper Tanana glosses, but was substituted by \textsc{num}.} This combined label allows the system to learn segmentation and glossing simultaneously. 

\pex   
\label{ex:JointInOut}
\a<a> \textbf{IN:} \hspace{6 mm} t \hspace{2 mm} a \hspace{2 mm} x \hspace{2 mm} e \hspace{2 mm} s 
\label{ex:Jointin}
\a<b> \textbf{OUT:} \hspace{2 mm} tax\#levy \hspace{3 mm} -es\#\textsc{pl} 
\label{ex:JointOut}
\xe

The sequential system involves two models. One model learns morpheme segments and the other learns glosses of the predicted morphemes. The first half of the data is used to train the segmentation step and the segments are predicted for the second half of the data as well as the test set. In the glossing step, it is assumed only these predicted segments have been glossed and can be used for training the sequential system. The output to the first model is a sequence of segments only, shown in (\ref{ex:Pipe1Out2In}). These predicted segments are used as input for the glossing model. The glossing model outputs predicted glosses for the predicted segments, as shown in (\ref{ex:Pipe2Out}). 

\pex  
\label{ex:Pipe1InOut}
\a<a> \textbf{SEGMENTATION IN:} \hspace{2 mm} t \hspace{2 mm} a \hspace{2 mm} x \hspace{2 mm} e \hspace{2 mm} s 
\label{ex:Pipe1in}
\a<b> \textbf{SEG. OUT / GLOSS. IN:} \hspace{2 mm} tax \hspace{3 mm} -es
\label{ex:Pipe1Out2In}
\a<d> \textbf{GLOSSING OUT:} \hspace{2 mm} levy \hspace{2 mm} \textsc{pl}
\label{ex:Pipe2Out}
\xe


\subsection{Feature-based vs. Deep Learning Models}
\label{sec:CRFvNN}

Since the mid-2010s, it has been reasonable to expect that deep learning models will outperform feature-based models on any NLP task, except with limited data. With the success of the Transformer \citep{vaswani_attention_2017} in low-resource settings, this expectation may hold true even in low-resource settings. This third study in segmentation and glossing compares the performance of feature-based and deep learning models. It repeats the experiments described in \autoref{sec:sgstrategies} and \autoref{sec:sgjoint}. The only difference is that feature-based models are used instead of the Transformer.

The joint approach to segmentation and glossing is carried out with the Conditional Random Fields (CRF) \citep{lafferty_conditional_2001}. The sequential approach uses two feature-based models: the CRF for segmentation and a multi-class linear Support Vector Machine (SVM) for glossing. 

The input of feature-based models is actually a list of features for each letter in a word. The features were chosen to be cross-linguistically applicable, plus an identical bias feature for each input. For the CRF in both the joint and sequential approach the features extracted are 1) the letter as represented in text, 2) the letter in lower case, 3) the whole word as represented in the text, 4) the whole word in lower case, 5) the length of the word as a number of characters, 6-7) position of letter in word counted from both last and first letter, 8-15) the 1-4 preceding and subsequent letters that surround the current letter. 

The sequential approach allows a richer set of contextual features for the glossing-only step. The input to the SVM is a list of features for each predicted morpheme segment. The first features are a concatenation of the above features for each letter in the predicted segment. Then morpheme-specific features are added. They are 1-2) surrounding morpheme, 3) the shape of the current morpheme. 

The CRF models gives a sequence of BIO-labels \citep{ramshaw1999} as output. Each character in the input is aligned with a Beginning-Inside-Outside (BIO) label. This is a type of tagging where each input token is declared either the beginning (B) of a morpheme or gloss, or the inside (I).\footnote{Since every letter is part of a morpheme the Outside (O) label is not needed as it would be in the BI0-labeling common application in Named Entity Recognition.} For the joint task, the BIO label includes the morpheme shape and its gloss, as shown in (\ref{ex:BIOlabelsjoint}). For the sequential task only the morpheme shape is included for the segmentation-only step, as shown in (\ref{ex:BIOlabelssegonly}). 

\pex   
\label{ex:BIOlabels}
\a<a> {\bf INPUT:} \hspace{3 mm}  a \hspace{2 mm}  v \hspace{2 mm} a \hspace{2 mm} y \hspace{2 mm} d \hspace{2 mm} i
\a<b> {\bf JOINT OUTPUT:} \hspace{.5 mm} B-ava\#\textsc{be} \hspace{.5 mm} I-ava\#\textsc{be}  \hspace{.5 mm} I-ava\#\textsc{be} \hspace{1 mm} B-d\#\textsc{ptp}  \hspace{.5 mm}B-i\#\textsc{sbst}  \hspace{.5 mm} I-i\#\textsc{sbst}
\label{ex:BIOlabelsjoint}
\a<c> {\bf SEGMENTATION OUTPUT:} \hspace{1 mm} B-\textsc{be} \hspace{1 mm} I-\textsc{be} \hspace{1 mm} I-\textsc{be} \hspace{1 mm} B-\textsc{ptp} \hspace{1 mm} B-\textsc{sbst} \hspace{1 mm} I-\textsc{sbst}
\label{ex:BIOlabelssegonly}
\xe

BIO-labeling is not used for the glossing-only because each input instance is a single morpheme and each output is a single gloss. The input and output is similar to that shown in (\ref{ex:Pipe1in}) and (\ref{ex:Pipe2Out}), except that there is only one morpheme per input instance. This allows the model to train much faster than when having one word per line and seems to improve results.

These feature-based models require the input and output sequences be of equal length so the number of predicted morpheme segments and the number of gold glosses must match during training of the glossing-only step. When they did not, the number was normalized by adding an `\textsc{unpredicted}' gloss for every extra predicted segment or by adding a ``NULL'' feature for every morpheme segment that was not predicted as it should have been. 


\section{Results}
\label{sec:sgresults}

The system predictions were automatically evaluated against the gold standard test set that withheld from the corpus. Only the five corpora that were annotated in FLEx could be used for all experiments but scores from the three other languages are included for comparison when available.\footnote{\textit{Because of a server crash, the Lamkang CRF model is still training. It will take about a week but should be completed before the April 13 deadline to submit the dissertation to the CU Graduate School. The joint and sequential Arapaho the sequential Southern Sierra Miwok results may or may not be completed by then.}} F$_1$-scores were calculated as a micro-average on all labels, rather than of each word. F$_1$-scores are used because they a better measure of success than mere accuracy when data is as imbalanced as segments and glosses are. The F$_1$-score provides a harmonic mean of precision (proportion of segments/glosses identified as $x$ that were correctly identified) and recall (proportion of segments/glosses that were correctly identified as $x$ out of those that should have been identified as $x$). 


\begin{table}[!tb]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
          & \multicolumn{4}{c|}{\textbf{Transformer}} & \multicolumn{4}{c}{\textbf{CRF (and SVM)}} 
          \\
          & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c|}{\textbf{Canonical}}  & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} 
          \\
          &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} \\
         \hline
         Alas & .4280 & .4565 & .5166 & .5291 
              & .5573 & .6319 & .5792 & \textbf{.6360} \\
          % difference & \multicolumn{2}{c}{.0285} & \multicolumn{2}{c}{.0125} \\
         \hline
         Arapaho & .7630 & \textbf{.7780} & - & - 
                 & - & - & - & -  \\
         \hline
         Lamkang & .7091 & .7391 & .5414 & .5785 
         &  & \textbf{.8376} &  & .8197 \\
         % difference & \multicolumn{2}{c}{.0300} & \multicolumn{2}{c}{.0371} \\
         \hline
         Lezgi  & .5489 & .6062 & .4993 & .5371 
                & .6696 & \textbf{.7090} & .6518 & .6888 \\
         % difference & \multicolumn{2}{c}{.0573} & \multicolumn{2}{c}{.0378} \\
         \hline
         Manipuri & .4719 & .5067 & .6401 & .6675 
                  & .7766 & .8063 & .7904 & \textbf{.8191} \\
         % difference & \multicolumn{2}{c}{.0348} & \multicolumn{2}{c}{.0274} \\
         \hline
         Natügu & .5423 & .5263 & .6083 & .6335 
                & .8388 & .8395 & .8349 & \textbf{.8398} \\
         % difference & \multicolumn{2}{c}{.0160} & \multicolumn{2}{c}{.0252} \\
         \hline
         Tsez & - & - & .8592 & \textbf{.8997} 
              & - & - & - & - \\
         \hline
         So. Sierra Miwok & .6848 & \textbf{.6982} & - & - 
                          & - & - & - & - \\
        % So. Sierra Miwok & .6848 & \textbf{.6982} & - & - 
                          %& .6501 & - & - & - \\
         \hline
         Upper Tanana & .7240 & .7849 & .7459 & .7886
                      & .7117 & .7942 & .7183 & \textbf{.7970} \\
    \end{tabular}
    \caption[Results of All Segmentation and Glossing Models]{F$_1$-scores of Transformer and CRF joint and Transformer+Transformer and CRF+SVM sequential models with both segmentation strategies. Transformer scores are an average across a 10-fold cross-validation. CRF and SVM are results of one run. The sequential approach (Seq) results are the average of the segmentation and glossing models' results. Best overall score for each language is bolded.}
    \label{tab:allsgresults}
\end{table}


\autoref{tab:allsgresults} displays all F$_1$-scores. On average all models achieved over 0.60 F$_1$-score. Only the smallest corpus, Alas [btz], barely scored above that; it only scored higher on the sequential approach with the feature-based models. The larger corpora (Lamkang and Manipuri) scored over 0.70 average F$_1$ score with the Transformer and over 0.84 with the feature-based models. The feature-based models gave scores just as high with Nat\"ugu but the Transformer results were noticeably lower.

The evaluation of the various tasks differed slightly. For the joint task, the scores indicate morphemes that were correctly segmented and glossed. For the sequential systems, the scores are an average of the scores from the segmentation and glossing models. The performance of the Transformer was evaluated by a cross-validation on ten random training/development sets with a 9/1 split from each half of the data used for the given experiment. The feature-based models were evaluated on a single run without a development set. 


\subsection{Surface vs. Canonical Results}

Both sequential and joint tasks (cf. \autoref{sec:sgjoint}) are trained on both strategies. The differences between surface and canonical segmentation in both the joint and sequential approaches are shown in Table \ref{tab:segdiffresults}. The effect of the segmentation strategy is roughly the same in both approaches. The differences almost disappear with the feature-based models. Doubling the training data affects the relative performance of the joint and sequential approaches.


\begin{table}[!tb]
    \centering
    \begin{tabular}{l|c|c|c|c}
          & \multicolumn{2}{c|}{\textbf{Transformer.}} & \multicolumn{2}{c}{\textbf{CRF (SVM)}} \\
          & \textbf{Joint} & \textbf{Seq} & \textbf{Joint} & \textbf{Seq} \\
         \hline
         Alas  & -.09 &  -.07 & -.02 & .00 \\
         Alas all & -.01 & - & - & - \\
         %Alas  & -.0886 &  -.0726 & -.0214 & -.0041\\
         %Alas all & -.0130 & - \\
         \hline
         Lamkang  & .17 & .16 &  &  \\
         Lamkang all & .13 & - & - & - \\
         %Lamkang  & .1677 & .1606 &  &  \\
         %Lamkang all & .1300 & - \\
         \hline
         Lezgi  & .05 & .07 & .02 & .02 \\
         Lezgi all & .01 & - & - & - \\
         %Lezgi  & .0496 & .0691 & .0178 & .0202 \\
         %Lezgi all & .0099 & - \\
         \hline
         Manipuri  & -.17 & -.16 & -.01 & -.01 \\
         Manipuri all & -.02 & - & - & - \\
         %Manipuri  & -.1682 & -.1608 & -.0138 & -.0128  \\
         %Manipuri all & -.0210 & - \\
         \hline
         Natügu  & -.07 & -.11 & .00 & .00 \\
         Natügu all & .00 & - & - & - \\
         %Natügu  & -.0660 & -.1072 & .0039 & -.0003\\
         %Natügu all & -.0030 & - \\
         \hline
         Upper Tanana & -.02 & .00 & -.01 & .00 \\
         Upper Tanana all & -.02 & - & - & - \\
         %Upper Tanana & -.0219 & -.0037 & -.0066 & -.0028 \\
         %Upper Tanana all & -.0159  & - \\
    \end{tabular}
    \caption[F$_1$-score Differences between Surface and Canonical Segmentation]{The F$_1$ differences between the average results on surface and canonical segmentation strategies with the Transformer. Positive numbers mean surface segmentation outperformed canonical segmentation.}
    \label{tab:segdiffresults}
\end{table}


Although the Transformer shows a bigger difference than the feature-based models but the general trend is the same. When comparing segmentation strategies languages with a higher ratio of unique labels to total tokens tend to do better with canonical segmentation. The differences are quite small for Alas [btz], Lezgi, and Nat\"ugu [ntu]. The biggest differences are found in Lamkang and Manipuri [mni], but their improvement goes in opposite directions. Surface segmentation gives higher scores for Lamkang data while Manipuri has higher scores with canonical. Interestingly, these two languages have the largest difference of the number of unique labels between surface and canonically segmented data. In Lamkang and Manipuri training data, the average number of unique joint labels increased by over 500 and 400, respectively, and in the segmentation step of the sequential system the number of segments increased by over 350. In the other languages the largest average increase of labels is 88 but usually the differences are less than 15. Since Lamkang and Manipuri belong to the same family, it is possible that significant differences in segmentation strategies are due to characteristics of their familial morphological structure, but it could be due to other factors such as idiosyncratic choices in the orthographic representation. 
It is also possible that the differences are due to similar annotation methods as the two corpora were sourced from the same origin. % Much less difference in number of labels in the glossing step: -12.1 - 7.2, unsurprisingly  

The segmentation strategies were also compared using all available data in the joint system with the Transformer.  When all the available data is used, the comparison of surface and canonical segmentation paints a clearer picture. The difference between the two strategies becomes less noticeable when the data is increased, but at the same time, canonical segmentation begins to outperform surface segmentation more consistently.  
Doubling the training data improves F$_1$-scores by about .2 to .4 points. The difference becomes quite small (roughly .1 points or less) with more data, closer to the differences with feature-based models with half the amount of data. 


\subsection{Joint vs Sequential Results}

Overall, the sequential system does better than the joint approach to segmentation and glossing, but the difference is not great. The average scores across Alas, Lamkang, Lezgi, Manipuri, Nat\"ugu, and Upper Tanana are shown in \autoref{tab:avgsgresults}. The best improvement with the Transformer is slightly over .06 points on Upper Tanana. The best improvement with feature-based models is .08, also on Upper Tanana. 

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
          & \multicolumn{4}{c|}{\textbf{Transformer}} & \multicolumn{4}{c}{\textbf{CRF (and SVM)}} 
          \\
          & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c|}{\textbf{Canonical}}  & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} 
          \\
          &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} \\
         \hline
         \textbf{Average} & .6090 & \textbf{.6370} & .6301 & \textbf{.6620} 
                 & .7007 & \textbf{.7697} & .7149 & \textbf{.7667} \\
         %Average (original 4 lgs) & .5400 & .5670 & .5011 & .5895 \\
    \end{tabular}
    \caption[Average Results of All Joint and Sequential models]{Average F$_1$-scores across all available languages on the Transformer and CRF joint and Transformer+Transformer and CRF+SVM sequential models with both segmentation strategies.}
    \label{tab:avgsgresults}
\end{table}

The performance on the Nat\"ugu data is the only case where the sequential system does not consistently improve over the joint system. The joint approach outperformed the sequential system on surface segmentation with the Transformer, but by only .016 points. However, the differences between the various experiments are so slight in Nat\"ugu compared to the other corpora that any system or strategy may perform nearly equally well on that language. Interestingly, the Nat\"ugu corpus has the smallest difference in the number of unique labels between surface and canonical segmentation (an increase of 14 labels, compared to next lowest of 46). With so few languages, it is difficult to say whether the relative number of unique labels significantly affect performance. More corpora should be included for this question to be explored further. 
 

\subsection{Feature-Based Results and Discussion}
\label{sec:sgFtrAnalysis}

The results of the deep learning and the feature-based models were compared. The differences between the CRF and the Transformer on joint approach and the differences between the CRF+SVM and Transformer+Transformer on the sequential approach are shown in \autoref{tab:DLFtrResults}. Transformer results were subtracted from the feature-based results. The feature-based models were were evaluated on a single run because less variation is expected and because the models take significantly longer to train. All Transformer results are reported on the average score from a 10-fold cross-validation. 

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|cc|cc}
        & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} \\
        & \textbf{Joint} & \textbf{Seq} & \textbf{Joint} & \textbf{Seq} \\
        \hline
        Alas & .13 & .18 & .06 & .11 \\
        %Alas & .1293 & .1754 & .0626 & .1069 \\
        \hline
        Lamkang &  & .10 &  & .24 \\
        %Lamkang &  & .0985 &  & .2412 \\
        \hline
        Lezgi & .12 & .10 & .15 & .15 \\
        %Lezgi & .1207 & .1028 & .1525 & .1517 \\
        \hline
        Manipuri & .30 & .30 & .15 & .15 \\
        %Manipuri & .3047 & .2996 & .1503 & .1516 \\
        \hline
        Nat\"ugu & .30 & .31 & .23 & .21 \\
        %Nat\"ugu & .2965 & .3132 & .2266 & .2063 \\
        \hline
        Upper Tanana & -.01 & .01 & -.03 & .01 \\
        %Upper Tanana & -.0123 & .0093 & -.0276 & .0084 \\
        \hline
        \hline
        Average & .21 & .20 & .28 & .17 \\
        %Average & .209725 & .19976 & .28195 & .17322 \\
    \end{tabular}
    \caption[F$_1$-score Differences of Feature-based Models minus Deep Learning]{The differences between the CRF or CRF+SVM models and the Transformer. Positive numbers means the feature-based model outperformed the Transformer.}
    \label{tab:DLFtrResults}
\end{table}

In all cases, except for Upper Tanana, the feature-based models outperformed the Transformer by as much as .31 F$_1$-score and as little at .01. On average, the feature-based models boost performance by about .2 points. If only these numbers are considered, the feature-based model would be clearly a better choice for integrating into language documentation and description. 

However, other factors need to be considered. Although feature-based models can still outperform a state-of-the-art deep learning model in low-resource settings, they are not necessarily always a better choice because they are more complicated to use. The set-up, feature extraction, and data formatting for feature-base models is significantly more difficult and time-consuming than for deep learning models. The deep learning models are more flexible. The features have to be determined and extracted. This time-consuming because of the extra coding and because different set of features should be tested for best results. 

%For example, some implementations of CRF require non-ASCII data be converted to ordinal numbers because it would not accept non-ASCII characters such as the Cyrillic font of Lezgi. 
%and the models take longer to train. For example, training and testing the Transformer 10 times on the larger corpora took about 2-3 days but the CRF needed about a week to run once. 

The complications are increased when dealing with the different segmentation strategies. For corpus with only canonical segmentation, the corpus does not already have surface segments by which morph breaks between letters can be located. This makes it extremely difficult to determine how to align the BIO-labels of each morpheme to input sequence of letters. This is another reason there are no Tsez results from the feature-based models. This alignment issue would be a problem for future work. 

Another complication arises from inconsistent segmentations. The CRF input and output sequences must be the same length, one label per letter, while also preserving the surface morph breaks. It turns out that the projects that only surface-segmented (Arapaho and Southern Sierra Miwok) did not always strictly follow that strategy. The surface morphs provide reference for morph breaks which are matched to the appropriate B or I label. When the number of characters in the transcribed word did not match the number of letters in the morphs, those words had to be eliminated during training. The number of words were roughly the same as the size of the development set for the Transformer 
%(arp: 4163; skd: 329). 


\subsection{Discussion of Deep Learning Results}
\label{sec:sgDLanalysis}

A closer look at the results of the Transformer models reveals interesting patterns. One significant factor in system performance is sparsity of data. Unsurprisingly, most errors occur on rarer forms. The larger class of stems means these are more often segmented/glossed incorrectly than affixes. Another factor is the amount of inconsistencies or errors in the manually annotated data. Poor annotation quality can amplify data sparsity. 

Allomorphy and isomorphy (same character sequence, different meaning) caused repeated errors during the glossing step and joint learning, where it becomes quite obvious that the model must deal with multiple options. For example, the Lezgi suffix \textit{-di}\footnote{In running text, Lezgi text is transliterated from the Cyrillic orthography for the reader's convenience.} has five possible glosses as shown by the joint labels in (\ref{ex:isomorphy}). These morphological phenomena are a moot issue during the segmentation step. 
%In Lezgi, this includes identical character sequences for aorist verbs and aorist participles.

\pex   
\label{ex:isomorphy}
-di\#\textsc{ent} \\
-di\#\textsc{dir} \\
-di\#\textsc{erg} \\
-di\#\textsc{obl} \\
-di\#\textsc{sbst} 
\xe

Sometimes multiple glosses are not due to morphological structure, but because the same morph(eme) was given different glosses. For example, interchanging `be' and `is' and `\textsc{cop}' for copular verbs or alternating between lexical glosses (e.g. `you') and grammatical glosses (e.g. `\textsc{2sg.erg}'). Sometimes different glosses appear because the item can be translated by different English words depending on the context. For example, one Lezgi word can be, and is, translated as `be' in some context or 'happen' in others. If alternative labels such as \textit{bahaye\#danger} and \textit{bahaye\#dangerous} are equally frequent, the model must choose randomly. Such inconsistency is to be expected from manual work and could be reduced with more automated assistance from machine learning.

Another pattern of errors is caused by tokens that were only partially segmented (and therefore, not correctly glossed). We knew that many such tokens were included in the gold standard data but there was no reliable way to eliminate them automatically. It is unclear how many exist in each corpus, although Alas and Nat\"ugu seem to have the least. Manipuri [mni] and Lezgi seem to have most incomplete segmentation. This became clear for Manipuri during another project when a language expert was asked to the correct the glosses for several inflected words. It appears that, in the data set, the annotators had been focused only on segmented and glossing certain morphemes on each word, leaving other affixes on the word unsegmented. 
The Lezgi data was annotated by a non-linguist who was trained to use FLEx and did not fully grasp Lezgi's unique morphology or simply did not finish segmenting all words. 

Many quality issues unpredictably increase the number of possible labels and amplify data sparsity. An example is
%did.again.ENT INESS did.again.ENT ENT - INESS is not on verbs
repeated mispelling of glosses (e.g. apperance---appereance---appearance, fourty---forty). Other misspellings originate in transcription. In the Lezgi test data, over 50 misspelled or incorrectly segmented strings were found in the first 200 hundred unique segments, although a few spelling changes are representation of dialectical variations.  

%Lezgi: Some of them should probably not be mulitple:  -bur --\textrangle{} ABS.PL/SBST.PL ==\textrangle{} PL,  -v --\textrangle{} AD/ADESS should be one or the other. 
%Lezgi: The multiple glosses per segment is clearly seen. As our misspellings pr variant spellings in original text:  \foreignlanguage{russian}{вуч вуш} 'what'. 

The results from the Alas corpus were quite good when compared to the much larger corpora. However, the errors are less predictable and more random. It seems likely that the small data set increased the noise to signal ratio and obscured general patterns. 
The Alas model is less likely to get rare morphemes correct. 
One noticeable confusion was caused by the canonical representation of circumfixes. This is shown in (\ref{ex:alaserrors}) where the model predicted a prefix \textit{n-}. This prefix is a correct surface allomorph of the circumfix at that position. 
The promiscuous attachment of Alas clitics also seem to difficult to learn.

\pex   
\label{ex:alaserrors}
\a \textbf{GOLD:} \hspace{2mm} n\textlangle{}\textrangle{}ken- \hspace{1mm} nindekh \hspace{1mm} -n\textlangle{}\textrangle{}ken \\
\textbf{OUTPUT:} \hspace{2mm} n- \hspace{5mm} nindekh \hspace{2mm} -n\textlangle{}\textrangle{}ken
\xe

%!Or confusion of stems, which of course are rarer: ke\textlangle{}\textrangle{}en\textrangle{}- kuase -\textlangle{}ke\textlangle{}\textrangle{}en ke\textlangle{}\textrangle{}en\textrangle{}- suasane -\textlangle{}ke\textlangle{}\textrangle{}en
%!Clitics are confusing presumably because they attach to any word in any position: khut -=ne khut

%variation of glosses that should probably be one gloss, perhaps due to context: -\textlangle{}n\textlangle{}\textrangle{}ken#CAUSATIVE -\textlangle{}n\textlangle{}\textrangle{}ken#TO.CAUSE.TO.BECOME..., bahaye#danger bahaye#dangerous

Nevertheless, error analysis shows that the models deal with data sparsity quite well. Even incorrect segments often have very similar character sequences to the correct choice, particularly when the difference is due to a change in the root vowel (e.g. dakhi $\sim$ dikhi). One of the most interesting errors, indicating the model's strong ability to learn patterns even in the face of data sparsity, occurred in Lezgi. The transcribed oral speech has a few dozen codeswitched Russian words. The test data include one or two examples, and in one case the model substituted one codeswitched word with another codeswitched word. 
%Codeswitched: poryadok okruzheniya -е
%\foreignlanguage{russian}{эвецӏун -на эвецӏна -на, хан хун}
%root vowel changes, especially this root 'say' which is quite common in narratives with reported speech:  \foreignlanguage{russian}{лугьун -на лагьа -на}. Also another very common root, 'go' which changes root vowel in difference conjugations:  \foreignlanguage{russian}{хъфена -на хъфин -на}
%Incorrect segmentation that the model corrected:  \foreignlanguage{russian}{i.arada -е i.arada, эхъвена -на экъвен -nа, ya ni ya -ni}
%Or just confused by incorrect segmentation in training data:  пӏузарар -ar -ar пӏузарар -ar -е, gada -di -z gadadiz, gadadin gаdа -di -n
%Confused by semantically motivated allomorphy of ERG/INESS stem:  \foreignlanguage{russian}{къачун -да къачу -е}
%Reduplication?:  \foreignlanguage{russian}{гьагьам гьам, гьа-гьа-гьа гьа-гьа}
%The models Chooses misspelling/variant spelling:  \foreignlanguage{russian}{и кьван икьван}

Many errors noted during error analysis were not actually errors. Since the annotation was originally done by hand, sometimes by multiple annotators, the glosses varied due to misspellings or synonomous glossing choices (e.g. 'BE.PST' vs. 'was'). There was a clear pattern in all datasets for one of the variants to be predicted rather than a random, unrelated label. These cases would not be considered errors by human annotators but were evaluated automatically as errors in the test data. For instance, one Lezgi demonstrative pronoun was sometimes glossed as `these' and sometimes as `this \textsc{-abs.pl}'. In at least once case, the second (and more linguistically precise) analysis was predicted. Unfortunately, because we did not have access to language experts for every corpora, we were not able to normalize our scores based on this knowledge; however, in the future it may be useful to consider that the performance of models trained on field data may, for all practical purposes, be better than the initial scores indicate.

In other cases, the labels in the test data were evaluated as errors, but closer examination revealed that the original human annotation were incorrect in that particular instance and the predicted label was actually the best fit to the data. So, an human error had been ``corrected''. 
%avaybur, avariydiz, avatna 2sg.erg - 2sg.abs - ERG.  
%Some words were never or inconsistently segmented in training data, or the model learned consistent patterns of inconsistent segmenting. Things like  abur -\textrangle{} just inconsistent segmentation/glossing mostly likely. Sometimes right because not always segmented in data (cf. case-stacking). 
Word instances that had been incorrectly segmented by the human annotators were sometimes correctly segmented by the model, although again these examples were evaluated as incorrect because they did not match the gold standard data. For Lezgi, these examples of ``correction'' by the model were more frequent in the sequential system, and may explain why biggest improvement by the sequential system over the joint system is found in the Lezgi data, which we know had many incorrect or incomplete segmentations.  Again, due to the lack of language experts, we are unable to say whether this holds true for all corpora but this should be explored deeper in future research.


\section{Conclusion}
\label{sec:sgconclusion}

This paper is aimed at smoothing the road to more interdisciplinary work with NLP and linguistics by articulating and examining the results of different research designs. Different research designs arise from different expectations or conventions in the two fields. Although they do not present barriers to mutually beneficial research, different expectations, such as in segmentation strategies, and different workflows, such as joint or separate segmentation/glossing, should not be dismissed when they arise. This paper tests the possible effects of these two differences.

The small difference between surface and canonical segmentation for three of the five languages suggests either strategy is a useful approach with minimal data, although this changes when data is increased in the joint model. Even though surface segmentation increases the number of labels in a dataset, this appears to be balanced by the by the abstract character of canonical morphemes, most noticeably by circumfixes. The fact that the difference almost disappears when the data size is doubled indicates that the question of segmentation strategy can be eliminated by simply annotating more data with whatever strategy suits the project at hand. 
However, larger differences on Lamkang and Manipuri corpora indicate that the reasons why segmentation strategies does sometimes differ in performance on the same corpus should be explored more across other Tibeto-Burman languages. Testing the differences in related languages might indicate whether certain linguistics features influence the results of different segmentation strategy when integrating NLP systems. 

The consistent improvement of the sequential system over joint learning may be a reason to consider separating segmentation and glossing tasks in order to leverage the higher accuracy of segments, and a more completely segmented corpus, when glossing the corpus. 
%However, This may not be practical during the earliest stages of analysis. 
%However, the focus of interlinearization changes once an initial analysis is settled upon; the primary goals become uncovering rare structures and increasing available resources. Machine learning is the best way to do both quickly. Also, 
The strength of the sequential system might be applied when a corpus cannot be completely segmented and glossed due to budget or time constraints. Instead, a strategy would be to prioritize segmenting and benefit from computational assistance when glossing. 

The feature-based models consistently outperformed the deep learning model by up to .3 F$_1$-score. This might be a reason for linguists and NLP practitioners to prefer feature-based models. However, better results need to be balanced against the ease of setting up and training a deep learning model like the Transformer. 

These studies could serve as a foundation towards more efficient use of computational methods in linguistic analysis and annotation. This paper shows, for example, that the glossing-only model performs well even on inaccurate segmentation predictions and can even ``correct'' manual segmentation errors. The study presented here assumes that the model's segmentation is not corrected by the language experts before training the glossing model. If a human-in-the-loop workflow was introduced to first correct segments, then the glossing-only model could improve even more. Such methodological considerations should be tested to see to what extent linguistic analysis and annotation of endangered language might benefit.

%However, the differences between the joint and sequential systems are not great.
%and other factors hint that linguists should adjust their approach to other things than workflows. 
Finally, as \citet{mcmillan-major_automating_2020} noted in glossing research, consistency of the annotations has a strong effect on system performance. This is most clearly seen in Lezgi which is known to be particularly noisy. Random strange characters were found at morpheme boundaries (e.g. {\tt *} instead of  {\tt -}). The human annotators frequently segmented one pair of characters whenever it occurred because it matched a frequent suffix. Allomorphs were frequently glossed as if they were different morphemes, undoing the benefit of canonical segmentation. Finally, its unique case-stacking caused confusion both to the human annotator and to the system results. Morphemes with several semantically-motivated allomorphs are (incorrectly) glossed one way when they serve as a single case marker and another way when they are one of several case markers.
%LEzgi: Missing intermedial affixes in case stacking:  imi -di -ni imni
%Confusing OBL and ERG. They are identical but OBL is tagged in case stacking (fairly consistently). 

So what would happen if linguists emphasized quality over quantity? We can answer this question by comparing Lezgi to Alas. According to the accounts of the linguists involved, and evidenced by our experimental results, the Alas data was annotated much more consistently and meticulously. With a corpus one third the size of the Lezgi corpus, the Alas model performs almost equally well. It is possible but seems unlikely that this is due to differing morphological structure. Unlike Lezgi---which is overwhelmingly suffixing and has fairly limited morphophonological changes---Alas features prefixing, suffixing, circumfixing, and infixing with various morphophonological processes. The main difficulty for the Alas systems was the sparsity of stems, compared to oft-repeated affixes. 

Interestingly, Alas showed the least marked preference between sequential and joint learning. This may indicate that higher consistency may eliminate the need to consider any change to segmentation/glossing workflow, but it should be investigated with further experiments focused on differences in annotation quality. Preferably these experiments would conducted on closely related languages to reduce effects due to different typology. 

%A downside to CRF in particular is that it takes so long to train. LMK and ARP run for over 200 hours with finishing. Whatever advantage may come from feature-based models has to be weighed against difficulty of formatting data and feature engineering. Lezgi and Lamkang took significantly longer to train on CRF. Because of Noise?


When considering low-resource settings, consistency for machine learning seems more important than data size, strategy, or workflow. Ruthless consistency is not something linguists have had reason to put high value on and it is not something to be expected by manual annotation, %although copying features and rule-based parsers in current software tools are helping.\mans{Copying? I don't follow this sentence.} 
Consistency can be provided by machine learning integration, but ironically, supervised machine learning needs high consistency in annotated data before it can perform accurately enough to assist human annotators by increasing their speed or accuracy. Our best estimate of the accuracy threshold for practical integration of machine learning into annotation is 60\% \citep{felt_improving_2012}. This threshold on F$_1$-scores was soundly passed by Lamkang because it over 18k manually annotated tokens for training but it was barely reached by the corpora with 4.5k-5.5k tokens. However, the meticulously annotated Alas corpus came close to this threshold with only 1.5k training tokens. If linguists wish to successfully integrate machine learning into the documentation and description of under-documented and endangered languages, then they must adopt from NLP an emphasis on highly consistent annotation.

%\section{Conclusion}

% The results indicate whether linguists should consider adjusting their work to NLP assumptions in order to optimize their efforts. Or whether NLP should consider the reality of working with linguistic resources. If they can expect that IGT data is both segmented and glossed, and not segmented without glosses, then they might want to adjust their experiments to this reality. Now, if a sequential method of first segmenting and then glossing gets better results, then the NLP experiments may take jointly segmented and glossed data and still choose sequential training to produce both. But if joint segmentation and glossing is a better way to leverage low-resource IGT data, then NLP researchers should consider re-framing their experiments in low-resource languages. NLP research should take into consideration real-world situation. Low-resource data is rarely going to be curated and carefully preprocess. There is not a lot of options if the data is not what was expected. NLP needs to be flexible and adapt to the noisy realities.

%NLP has made it possible to achieve more with less. Methods for low-resource languages are improving. These methods are slowing being introduced to linguistics. Some have been applied to language documentation tasks. However, these experiments make assumptions that are not based on a understanding of linguistics methods.
%Building systems that integrate well with linguistic conventions while also feeding NLP development effectively will undoubtedly require adjustments on both sides, but we should try to minimize those adjustments and optimize our work so that more languages can be documented/described and more communities can have equitable access to language technology. 

%Linguists segment and gloss a word before segmenting another next word.
%Supervised NLP systems have assumed the two tasks are completed independently. If NLP systems perform better by completing these task separately, then linguists may get optimal benefit by following this workflow. However, it would be better if NLP systems could adjust to the linguist's traditional workflow, if possible. If joint segmentation and glossing performs as well or better than a sequential model, then joint models should be preferred. 

%Linguists prefer canonical segmentation, but may surface segmentation is also used, and may be more common in data processed by Toolbox. 
%NLP systems will either prefer one, in which case linguists may need to adjust their workflow (e.g. do surface segmentation and glossing first, finish the texts with automated assistance, then return to do canonical segmentation, hopefully automated and presumably with the assistance from completed surface segmentation and glosses), or NLP do prefer one over the other, in which case linguists can integrate segmentation models into their work without adjustment. 

%A key to NLP success in low-resource settings is making the most of whatever resources do exist. It is important that automated segmentation/glossing systems integrate easily into linguists' workflow because, to achieve high accuracy (over 90\%) on limited data, NLP must eventually bring humans back into the loop.
%Integrating NLP into language documentation will mostly involve active learning. As Palmer et al. (??) showed, active learning must take the annotator into account to be effective. NLP methods should take the annotator/linguist into account in order for those methods to be optimally effective in the long-term. Automated work does not have this constraint if it assumes no active learning scenario. But that limits its usefulness to language documentation. With limited data, machine learning models cannot be expected to achieve very high accuracy. Its mistakes will have to be corrected by an annotator and and efficient combination of manual work and computer algorithm is to leverage those corrections to quickly improve the model with some sort of human-in-the-loop method, such as active learning.