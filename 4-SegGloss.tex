\chapter{Automated Segmentation and Glossing for Documentary and Descriptive Linguistics}
\label{chap:seggloss}

This chapter examines how variations in research design affect the integration of machine learning into the tasks for morpheme segmentation and glossing for under-documented languages. Morpheme segmentation and glossing are traditionally the first tasks undertaken after transcription. Both segmentation and glossing provide essential linguistic information upon which deeper analysis can be done. Segmenting words into morphemes can reduce confusion in NLP models caused by data sparsity because it clarifies relationships between word forms. Glosses make implicit morphosyntactic structures explicit and accessible for analysis. Segmented words and glosses can be leveraged to improve NLP models in low-resource settings, such as for machine translation \citep{shearing_improving_2018,zhou_using_2020}. Therefore, automating these tasks and integrating the automation as assistance for documentary and descriptive work will benefit both linguistics and NLP.

When we bring together two disciplinary fields for mutual benefit, different expectations or accepted conventions are also brought together. These expectations may seem to clash. This chapter addresses potential clashes that stem from differing approaches to the same tasks in natural language processing (NLP) and linguistic analysis. The approaches are based or have led to differing expectations about conventional methods or available data. For example, it is generally expected in NLP that a main goal is to process textual data that is an orthographic representation, whereas linguists may prefer to work with morphophonological representations and expect that the goal is to process underlying linguistic forms. These differences can make interdisciplinary collaboration unnecessarily complicated or perplexing. 

When these differences affect overall research design, it is easy to simply choose one or the other convention without testing which choice might actually benefit the task at hand or might be more efficient for long-term goals. This chapter compares the short-term affect of three pairs of differing expectations which have arisen during the authors' research. 
This first study examines \textit{how a choice of morpheme segmentation strategies affect NLP performance}. Linguistic theory assumes the existence of underlying, or canonical, morpheme forms and for linguists their segmentation strategy choice is guided by the goal to discover these forms. Canonical segmentation represent morphemes in their theoretical, underlying forms, which also allows orthographic changes triggered by surrounding phones to be ignored. This contrasts with surface segmentation which simply inserts morpheme breaks in the orthographic representation. Since NLP almost always deals with orthographic representations, its systems are trained to perform surface segmentation almost exclusively. 
It might seem reasonable that linguists who want to integrate automated assistance would adjust their strategy to match NLP expectations. But without testing, are we sure that NLP systems perform better at surface or at canonical segmentation?

The second study asks \textit{whether morpheme segmentation and glossing should be approached jointly or sequentially}. In other words, is an NLP system trained to do segmentation and glossing simultaneously as a joint task better than a system trained to treat them as two separate tasks? Instead of arbitrarily choosing one or the other method, we test whether one approach achieves more accurate results. If the sequential approach is more accurate, then linguists might want to consider adjusting their workflow in order to gain optimal benefit from NLP integration, but if the joint task approach performs better, then perhaps NLP would benefit by adjusting experiments to match how the linguists' produce new language data.

The third pair of differing expectations is not between linguistics and NLP but within NLP. The study looks at \textit{whether the Transformer model can outperform feature-based models.} Until recently, non-neural models regularly outperformed neural models in low-resource settings. For example, previous work on Lezgi \citep{moeller_automatic_2018} used the same corpus as the current work does and found the CRF outperformed the then state-of-the-art LSTM with reported F$_1$-scores of nearly .9.\footnote{A direct comparison cannot be made to this work because only affixes were glossed while the current work also glosses roots.} This expectation is becoming less sure as methods for deep learning models improve. It is still important to test the two types of models. Linguists may want to know what models to recommend when they approach computer scientists for collaborative work. Also, if feature-based models are consistently more accurate at the task, then both linguists and NLP scientists will want to know what features can be extracted for good results in any language. 

This rest of this chapter describes experiments that test machine learning segmentation and glossing systems and then compare those results between a joint and sequential approach to segmentation and glossing and between a surface and canonical strategy to segmentation and between feature-based and deep learning models.  The experiments are described in \autoref{sec:sgmethodology}. The results are presented in \autoref{sec:sgresults}. Only five of the corpora presented in Chapter \ref{chap:datamodels} could be used for all experiments because they were annotated in FLEx and so have both surface and and canonical morpheme segments: Alas, Lamkang, Lezgi, Manipuri, Nat\"ugu, Upper Tanana. These languages are shown in \autoref{tab:segglossdata} and only their results are considerd in the analysis and discussion in \autoref{sec:sgerrors}.


\begin{table}[!tb]
    \centering
    \begin{tabular}{l|r|rc}
         \textbf{Language} & \textbf{Tokens} & \multicolumn{2}{c}{\textbf{Seg/Gloss}} \\
         \hline
         Alas & 4.5k & 3,775 & 85\%  \\
         \hline
         Lamkang & 101k & 49,699 & 49\% \\
         \hline
         Lezgi & 14k & 13,353  & 94\% \\
         \hline
         Manipuri & 12k & 11,907 & 98\% \\
         \hline
         Natügu & 16.5k & 12,435 & 75\%  \\
         \hline
         Upper Tanana & 17.5k & 11,867 & 67\% 
    \end{tabular}
    \caption[Data for Segmentation and Glossing Experimentation]{The approximate total token considers multiple word expressions (when parsed as such) as single tokens. The percentage and total number of tokens that are both segmented (canonical and surface) and glossed are shown.}
    \label{tab:segglossdata}
\end{table}




\section{Experiments}
\label{sec:sgmethodology}

For the segmentation and glossing experiments, the data was arranged so as to accommodate both joint and sequential learning. That is, after withholding ten percent of the corpus as a test set, the remaining data was split into two equal training sets.  Ten percent of each part was used as a development set. For easier comparison, the joint model was trained on only one part, the same part used for training the segmentation step in the sequential system. One additional experiment was run with the joint model that trained on both parts together, minus the held-out data. For each experiment with the Transformer model, a ten-fold cross validation was run. Since the CRF took significantly longer to train, the feature-based models were run only once.

It was assumed that the documentary and descriptive field data had only been morpheme segmented and glossed. No other information was leveraged from the IGT or other resources. Gold standard data was created by filtering out tokens that were not completely segmented (both canonical and surface) or glossed, as far as could be determined. This was determined by assuring that the surface, canonical, and gloss lines aligned with each other. 
Glosses were standardized by capitalizing affix glosses. When morphemes had multiple English words or symbols in their glosses, the words were joined by periods.
Morpheme boundary markers such as hyphens ( - ) and equal signs ( = ) were preserved to distinguish clitics from bound morphemes and to indicate relative ordering of morphemes (i.e. pre-/suf/infixing); angle brackets ( \textlangle{}\textrangle{} ) were used to denote circumfixes. 

All tasks are treated as a problem of converting an input sequence of characters $\vect{x} = (x_1, \ldots, x_n)$ to an output sequence of labels $\vect{y} = (y_1, \ldots, y_n)$. The output sequence of labels indicate the (canonical or surface) morpheme and/or the morpheme's gloss. 
%Pilot work showed that any context provided by training on a whole sentence confused the system more than helped it, so each data instance is a word. 

\subsection{Surface vs. Canonical Segmentation Strategies}
\label{sec:sgstrategies}

The second experiment compares the Transformer's performance when trained on different segmentation strategies. Both systems described above are trained on both strategies. Canonical segmentation gives more information about a language's underlying morphological structure. At the same time, it reduces the number of unique labels in languages that reflect allomorphy and morphophonological processes in the orthography. On the other hand, surface segmentation does not require computational models to learn allomorphy or morphophonology \citep{goldsmith_computational_2017} and does not provide a thorough analysis of the language's morphology by annotators. It simply divide strings of text into segments known as ``morphs'' \citep{virpioja_empirical_2011} without regard to potential relationships between the segments. 

The two segmentation strategies are compared in (\ref{ex:canseg}) where the first two surface letters of each word in (\ref{ex:canseg1}) are represented by identical canonical segments in (\ref{ex:canseg2}).%Linguists may also choose a surface segmentation strategy, which is also the strategy selected by segmentation algorithms that simply identify morpheme boundaries between characters in orthographic representations. 
In practice, both strategies are encountered during language documentation and description, the initial strategy depending in part on software tools. For example, the older, but still popular, Toolbox\footnote{https://software.sil.org/toolbox/} allows surface segmentation whereas ELAN \citep{auer_elan_2010} supports both but as separate tasks, while FLEx \citep{baines_overview_2018} requires surface segmentation but facilitates simultaneous canonical segmentation. 


\pex   
\label{ex:canseg}
\a il-legal \hspace{6mm} in-capable \hspace{5mm} im-mature
\label{ex:canseg1}
\a in-legal \hspace{5mm} in-capable \hspace{5mm} in-mature
\label{ex:canseg2}
\a \textsc{neg}-legal \hspace{1mm} \textsc{neg}-capable \hspace{1mm} \textsc{neg}-mature
\label{ex:canseg3}
\xe

%This seems more natural since linguists are segmenting based on their understanding of the morphemes function/meaning, so in most cases a morpheme is segmented only because its gloss has been identified. So an automated system that assumes the steps occur consequently instead of simultaneously/jointly may not work well for linguists. It may be ever be accepted or may be difficult to integrate. It may make it difficult for linguists to benefit from automated assistance. On the other hand, this assumption of separate segmentation and glossing may point linguists to a better way. Linguists may benefit by integrating this NLP assumption into their work. 

The intention of this study is not to provide a direct comparison, since technically the corpora of surface and canonical segments are different datasets. The study assumes that if one strategy was conducted first, then the other type of segmentation might be more easily learned from it. For example, if a corpus could be surface segmented very quickly with very high accuracy based on initial hypotheses of morpheme shapes, then having the predicted surface segments for the whole corpus might make the discovery of canonical, underlying morphemes easier and faster for linguists, as well as matching a common expectation in NLP. 

For this study, only the corpora that had been interlinearized with FLEx was selected:  Alas, Lamkang, Lezgi, Manipuri, Nat\"ugu, Upper Tanana. This is because FLEx allows the annotators to provide both surface and canonical segments.
Even though the projects employed the same software tool the corpora had some differences in formatting. For example, in FLEx only one part of the circumfix is labeled as circumfix, the other part as prefix or affix. Both Natugu and Bahasa Alas were annotated for circumfixing, but in one corpus the prefixed part was labeled as a circumfix and the suffixed part as a suffix while the other corpus took the opposite approach. These kinds of variations had to be identified and preprocessed

The difference in the methodology of the two strategies is their outputs. Their input does not change and it is the same as the models described in section \ref{sec:sgjoint}. The output for surface segmentation is shown (\ref{ex:surfout}), and the corresponding output for canonical segmentation is in (\ref{ex:CanOut}).

\pex   
\label{ex:CanInOut}
\a<a> \textbf{SURFACE:} \hspace{2 mm} tax\#levy \hspace{3 mm} -es\#\textsc{pl}
\label{ex:surfout}
\a<b> \textbf{CANON.:} \hspace{4 mm} tax\#levy \hspace{3 mm} -s\#\textsc{pl} 
\label{ex:CanOut}
\xe

In addition to the alternation between surface morphs and underlying morpheme representations, the data was handled slightly differently for the two strategies. The most obvious difference is the handling of circumfixes. Surface representation only preserves the ordering of morphs and does not require knowledge of morpheme types, so the two parts of each circumfix were treated as two different prefix and suffix morphs.  Canonical segmentation represents the circumfix as a single morpheme that repeats before and after the stem. These changes are shown in (\ref{ex:Circumfixes}).

\pex   
\label{ex:Circumfixes}
\a<a> \textbf{SURFACE:} \hspace{2 mm} ke- \hspace{4 mm}  STEM  \hspace{1 mm} -en
\label{ex:circumsurf}
\a<b> \textbf{CANON.:} \hspace{1 mm} ke\textlangle{}\textrangle{}en- \hspace{1 mm} STEM \hspace{1 mm} -ke\textlangle{}\textrangle{}en
\label{ex:circumcan}
\xe

%Null morphemes should have been handled differently, but they weren't! They should have been eliminated it was discovered they remained because the selection of labels was based on the gloss line.


\subsection{Joint vs. Sequential Segmentation and Glossing}
\label{sec:sgjoint}

The first experiment tested whether joint or sequential segmentation and glossing is a better approach to interlinearization when integrating automated assistance. Joint segmentation assumes that segmented data without glosses is unlikely because identifying a morpheme usually means there has already been an identification of the morpheme's meaning.
%\footnote{One linguist claimed that native speakers could segment morphemes without knowing the function of morphemes (p.c) but in the first author's experience this results in many segmentation errors.} 
Joint segmentation requires the model to learn the morpheme boundary and gloss simultaneously for each segment. The sequential system--glossing after segmenting the whole text---assumes that segmentation is easier to do by hand or that unsupervised segmentation tools such as Morfessor \citep{smit-etal-2014-morfessor} are available for low-resource languages. 

For joint learning, the input is a character-level representation of a word, shown in (\ref{ex:Jointin}). Each character is treated as as separate symbol by the model. The output is a sequence of labels, one label per morpheme, shown in (\ref{ex:JointOut}). The label combines the morpheme's shape and gloss. The combination allows the system to perform segmentation and glossing simultaneously. 

\pex   
\label{ex:JointInOut}
\a<a> \textbf{IN:} \hspace{6 mm} t \hspace{2 mm} a \hspace{2 mm} x \hspace{2 mm} e \hspace{2 mm} s 
\label{ex:Jointin}
\a<b> \textbf{OUT:} \hspace{2 mm} tax\#levy \hspace{3 mm} -es\#\textsc{pl} 
\label{ex:JointOut}
\xe

The sequential system trains two models: one model learns morpheme segments and the other learns to gloss the predicted morphemes. In the sequential system the first equal part of the data was used for the segmentation step and its output was the training input for the glossing step. In the sequential apporach, it is assumed that segments and glosses exist for the first part of the data and can be used for training in the sequential system, but not for the second part. The output to the first model is a sequence of segments only, shown in (\ref{ex:Pipe1Out}). 

\pex  
\label{ex:Pipe1InOut}
\a<a> \textbf{IN:} \hspace{6 mm} t \hspace{2 mm} a \hspace{2 mm} x \hspace{2 mm} e \hspace{2 mm} s 
\label{ex:Pipe1in}
\a<b> \textbf{OUT:} \hspace{2 mm} tax \hspace{3 mm} -es
\label{ex:Pipe1Out}
\xe

The output of the segmentation model is used as input to the second model, as shown in (\ref{ex:Pipe2In}.) The glossing model then outputs the predicted glosses, shown in (\ref{ex:Pipe2Out}). 

\pex  
\label{ex:Pipe2InOut}
\a<a> \textbf{IN:} \hspace{6 mm} tax \hspace{3 mm} -es
\label{ex:Pipe2In}
\a<b> \textbf{OUT:} \hspace{2 mm} levy \hspace{2 mm} \textsc{pl}
\label{ex:Pipe2Out}
\xe

\subsection{Feature-based vs. Deep Learning Models}
\label{sec:CRFvNN}

Since the mid-2010s, it is reasonable to expect that deep learning models will outperform feature-based models. With the advent of the Transformer \citep{vaswani_attention_2017}, it even seems reasonable to expect this in low-resource settings, although deep learning models are more dependent on very large amounts of training data. This third study compares the performance of feature-based and deep learning models on segmentation and glossing. It repeats the experiments described in \autoref{sec:sgjoint} and \autoref{sec:sgstrategies}, but this time with feature-based models.

The joint task is performed with a Conditional Random Fields (CRF) model \citep{lafferty_conditional_2001}. The sequential approach uses the CRF for segmentation and a multi-class linear Support Vector Machine (SVM) for glossing.
The sequence model has an input consisting of individual characters and the output of a sequence of BIO-labels \citep{ramshaw1999}. That is, all tasks are treated as a labeling problem of converting an input sequence of letters $\vect{x} = (x_1, \ldots, x_n)$ to an output sequence of symbols $\vect{y} = (y_1, \ldots, y_n)$.  

\paragraph{BIO-labeling.} 
In the training data, each letter is associated with a Beginning-Inside-Outside (BIO) tag---a type of tagging where each letter position is declared either the beginning (B) of a morpheme or gloss, or the inside (I).\footnote{In all tasks, every letter are part of a morpheme, therefore the Outside (O) tag is not needed like it would be in Named Entity Recognition.} The BIO tags includes the morpheme shape and its gloss for the joint task, as shown in (\ref{ex:BIOlabelsjoint}), or the morpheme shape only for the segmentation only task, as shown in (\ref{ex:BIOlabelssegonly}). 

\pex   
\label{ex:BIOlabels}
\a<a> {\bf INPUT:} \hspace{5 mm}  a \hspace{10 mm}  v \hspace{10 mm} a \hspace{10 mm} y \hspace{10 mm} d \hspace{10 mm} i
\a<b> {\bf JOINT OUTPUT:} \hspace{.5 mm} B-ava\#\textsc{be} \hspace{.5 mm} I-ava\#\textsc{be}  \hspace{.5 mm} I-ava\#\textsc{be} \hspace{1 mm} B-d\#\textsc{ptp}  \hspace{.5 mm}B-i\#\textsc{sbst}  \hspace{.5 mm} I-i\#\textsc{sbst}
\label{ex:BIOlabelsjoint}
\a<c> {\bf SEGMENTATION OUTPUT:} \hspace{1 mm} B-\textsc{be} \hspace{1 mm} I-\textsc{be} \hspace{1 mm} I-\textsc{be} \hspace{1 mm} B-\textsc{ptp} \hspace{1 mm} B-\textsc{sbst} \hspace{1 mm} I-\textsc{sbst}
\label{ex:BIOlabelssegonly}
\xe

BIO-labeling is not used for the glossing only task because each input instance is a single morpheme and each output is a gloss. The input and output is similar to that shown in (\ref{ex:Pipe2InOut}). The difference is that for the CRF+SVM there is only one morpheme per input. This allows the model to train much faster than when having one word per line.

\paragraph{Sequential Approach: CRF+SVM.}
The sequential approach first segments and then glosses using separate models for each step. In feature-based models, this allows a richer set of contextual features for the each step. 
The CRF is employed only for segmentation. After predicting BIO-labels for morpheme shapes, the predicted morpheme strings are used as input to the SVM that uses features of the segmented morphemes to train the glossing step. 

The CRF input is actually a list of features for each letter in a word. The features were chosen to be cross-linguistically applicable. Besides a bias feature, the features extracted are 1) the letter as represented in text, 2) the letter in lower case, 3) the whole word as represented in the text, 4) the whole word in lower case, 5) the length of the word as a number of characters, 6-7) position of letter in word counted from both last and first letter, 8-15) the 1-4 preceding and subsequent letters that surround the current letter.

The input to the SVM is a list of features for each predicted morpheme segment. The first features are a concatenation of the above features for each letter in the predicted segment. Then morpheme-specific features are added. They are 1-2) surrounding morpheme, 3) the shape of the current morpheme. 

These feature-based models require input and output sequences of equal lenght, so the number predicted morpheme segments and the number of gold glosses for training must match. The number is normalized by adding an `UNPREDICTED' gloss for every extra predicted segment or by adding a `NULL' feature for every morpheme segment that was not predicted as it should have been. 

\section{Results}
\label{sec:sgresults}

The system predictions were automatically evaluated against the gold standard. Scores were calculated as a micro-average on all labels, independent of word accuracy. Since the system may predict more or fewer labels for a word, both precision and recall are calculated. 

The evaluation of deep learning and feature-based models differed slightly. The feature-based models were evaluated on a single run without a development set. The performance of the Transformer on all tasks was all evaluated by a cross-validation on ten training and development sets that were randomly split from the half of the data used for each experiment. Table \ref{tab:allsgresults} displays all F$_1$-scores. Although only the corpora that were annotated in FLEx could be used for all studies, scores on the three additional languages were added for comparison when available.
For joint learning, the scores indicate morphemes that were correctly segmented and glossed. For the sequential system, the score is a weighted average of the scores from both the segmentation and glossing models. 
%Variation of scores for all systems and strategies were very similar.

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
          & \multicolumn{4}{c|}{\textbf{Transformer}} & \multicolumn{4}{c}{\textbf{CRF (and SVM)}} 
          \\
          & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c|}{\textbf{Canonical}}  & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} 
          \\
          &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} \\
         \hline
         Alas & .4280 & .4565 & .5166 & .5291 
              & .5573 & .6319 & .5792 & \textbf{.6360} \\
          % difference & \multicolumn{2}{c}{.0285} & \multicolumn{2}{c}{.0125} \\
         \hline
         Arapaho & .7630 & .7780 & n/a & n/a 
                 &  &  & n/a & n/a  \\
         \hline
         Lamkang & .7091 & .7391 & .5414 & .5785 
         &  & \textbf{.8376} &  & .8197 \\
         % difference & \multicolumn{2}{c}{.0300} & \multicolumn{2}{c}{.0371} \\
         \hline
         Lezgi  & .5489 & .6062 & .4993 & .5371 
                & .6696 & \textbf{.7090} & .6518 & .6888 \\
         % difference & \multicolumn{2}{c}{.0573} & \multicolumn{2}{c}{.0378} \\
         \hline
         Manipuri & .4719 & .5067 & .6401 & .6675 
                  & .7766 & .8063 & .7904 & \textbf{.8191} \\
         % difference & \multicolumn{2}{c}{.0348} & \multicolumn{2}{c}{.0274} \\
         \hline
         Natügu & .5423 & .5263 & .6083 & .6335 
                & .8388 & .8395 & .8349 & \textbf{.8398} \\
         % difference & \multicolumn{2}{c}{.0160} & \multicolumn{2}{c}{.0252} \\
         \hline
         Tsez & n/a & n/a & .8592 & \textbf{.8997} 
              & n/a & n/a & n/a & n/a \\
         \hline
         So. Sierra Miwok & .6848 & .6982 & n/a & n/a 
                          &  &  & n/a & n/a \\
         \hline
         Upper Tanana & .7240 & .7849 & .7459 & .7886
                      & .7117 & .7942 & .7183 & .7970 \\
    \end{tabular}
    \caption[Results of All Segmentation and Glossing Models]{F$_1$-scores of Transformer and CRF joint and Transformer+Transformer and CRF+SVM sequential models with both segmentation strategies. Transformer scores are an average across a 10-fold cross-validation. CRF and SVM are results of one run. The sequential approach (Seq) results are the average of the segmentation and glossing models' results.}
    \label{tab:allsgresults}
\end{table}


\subsection{Surface vs. Canonical Results}

When half of the total data is used, the comparison of surface and canonical segmentation paints a less clear picture. The differences when going from surface to canonical segmentation are shown in Table \ref{tab:segdiffresults}. The general trend when comparing segmentation strategies is that languages with a higher ratio of unique labels to total tokens do better with canonical segmentation. The differences are quite small for Alas [btz], Lezgi, and Nat\"ugu [ntu]. The biggest differences are found in Lamkang and Manipuri [mni], but their improvement goes in opposite directions. Surface segmentation gives higher scores for Lamkang data while Manipuri has higher scores with canonical. Interestingly, these two languages have the largest difference of the number of unique labels between surface and canonically segmented data. In Lamkang and Manipuri training data, the average number of unique joint labels increased by over 500 and 400, respectively, and in the segmentation step of the sequential system the number of segments increased by over 350. In the other languages the largest average increase of labels is 88 but usually the differences are less than 15. Since Lamkang and Manipuri belong to the same family, it is possible that significant differences in segmentation strategies are due to characteristics of their familial morphological structure, but it could be due to other factors such as idiosyncratic choices in the orthographic representation. 
%It is equally possible that the differences are due to shared annotation methods as the two corpora were sourced from one origin.
% Much less difference in number of labels in the glossing step: -12.1 - 7.2, unsurprisingly  

The differences in the results in both joint and sequential systems are shown in Table \ref{tab:segdiffresults}. The effect of the segmentation strategy is roughly the same in both systems. 

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|r|r}
          & \textbf{Joint} & \textbf{Seq} \\
         \hline
         Alas  & -.09 &  -.07  \\
         Alas all & -.01 & n/a \\
         %Alas  & -.0886 &  -.0726  \\
         %Alas all & -.0130 & n/a \\
         \hline
         Lamkang  & .17 & .16  \\
         Lamkang all & .13 & n/a \\
         %Lamkang  & .1677 & .1606  \\
         %Lamkang all & .1300 & n/a \\
         \hline
         Lezgi  & .05 & .07  \\
         Lezgi all & .01 & n/a \\
         %Lezgi  & .0496 & .0691  \\
         %Lezgi all & .0099 & n/a \\
         \hline
         Manipuri  & -.17 & -.16   \\
         Manipuri all & -.02 & n/a \\
         %Manipuri  & -.1682 & -.1608   \\
         %Manipuri all & -.0210 & n/a \\
         \hline
         Natügu  & -.07 & -.11  \\
         Natügu all & .00 & n/a \\
         %Natügu  & -.0660 & -.1072  \\
         %Natügu all & -.0030 & n/a \\
         \hline
         Upper Tanana & -.02 & .00 \\
         Upper Tanana all & -.02 & n/a \\
         %Upper Tanana & -.0219 & -.0037 \\
         %Upper Tanana all & -.0159  & n/a \\
    \end{tabular}
    \caption[F$_1$-score Differences between Surface and Canonical Segmentation]{The F$_1$ differences between the average results on surface and canonical segmentation strategies with the Transformer. Positive numbers mean surface segmentation outperformed canonical segmentation.}
    \label{tab:segdiffresults}
\end{table}


The segmentation strategies were also compared using all available data in the joint system. Table \ref{tab:segdiffresults} shows the how doubling the training data affects the relative performance of the joint and sequential approaches. 
%Doubling the training data always improves F$_1$-scores by about .2 to .4 points. 
While the difference between the two strategies becomes less noticeable when the data is increased, canonical segmentation tends to outperform surface segmentation more consistently. Nevertheless, in all languages the difference between the strategies becomes quite small (.13 points or less) with more data. 

\subsection{Joint vs Sequential Results}

Overall, sequential learning does better than joint learning, but the differences are not great. The maximum improvement is less than 0.06 points on Lezgi [lez]. On average all models achieved over 0.60 F$_1$. Only the smallest corpus, Alas [btz], did not achieve a score above that with both the Transformer and the feature-based models, although it did achieve it with the feature-based models on the sequential approach. Lamkang [lmk], which has the largest number of tokens by far, achieved over 0.70 average F$_1$ score with the Transformer and nearly 0.84 with the feature-based models. The feature-based models had scores that high Nat\"ugu but the Transformer results were lower.

\begin{table}[!tb]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
          & \multicolumn{4}{c|}{\textbf{Transformer}} & \multicolumn{4}{c}{\textbf{CRF (and SVM)}} 
          \\
          & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c|}{\textbf{Canonical}}  & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} 
          \\
          &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} &  \textbf{Joint} & \textbf{Seq} \\
         \hline
         \textbf{Average} & .6090 & .6370 & .6301 & .6620 
                 &  &  &  & .7667 \\
         %Average (original 4 lgs) & .5400 & .5670 & .5011 & .5895 \\
    \end{tabular}
    \caption[Average Results of All Joint and Sequential models]{Average F$_1$-scores across all available languages on the Transformer and CRF joint and Transformer+Transformer and CRF+SVM sequential models with both segmentation strategies.}
    \label{tab:allsgresults}
\end{table}

The performance on the Nat\"ugu data is the only case where the sequential system is not consistently an improvement over the joint system. 
%With surface segmentation, joint learning on Nat\"ugu outperforms sequential learning. 
When considering word-level accuracy, Nat\"ugu joint learning outperformed sequential learning on canonical segmentation. Interestingly, it also has the smallest change in the number of unique labels between surface and canonical segmentation (an increase of 14 labels, compared to next lowest of 46). With so few languages, it is difficult to say whether the relative number of unique labels affect the relative performance when trained on surface vs. canonical segmentation. More corpora should be included for this question to be explored further. %Overall, the differences are still so slight that it seems any system or strategy may perform equally well on Nat\"ugu. 
 

\section{Discussion of Deep Learning Results}
\label{sec:sgerrors}

A closer look at the results of the Transformer models reveals interesting patterns. One significant factor in system performance is sparsity of data. Unsurprisingly, most errors occur on rarer forms. 
%The larger class of stems means these are more often incorrect than their affixes. %\foreignlanguage{russian}{гьакъикъет -е -ай кьисметда -е -ай, тан -зва -й -ди атун -зва -й -ди}
Another factor is the amount of inconsistencies or errors in the manually annotated data. Annotation quality can amplify data sparsity. 

Allomorphy and isomorphy (same character sequence, different meaning) caused repeated errors during the glossing step and joint learning, where it becomes quite obvious that the model must deal with multiple options. For example, the Lezgi suffix \textit{-di}\footnote{In running text, Lezgi text is transliterated from the Cyrillic orthography for the reader's convenience.} has five possible glosses as shown by the joint labels in (\ref{ex:isomorphy}). These morphological phenomena are a moot issue during the segmentation step. 
%In Lezgi, this includes identical character sequences for aorist verbs and aorist participles.

\pex   
\label{ex:isomorphy}
-di\#\textsc{ent} \\
-di\#\textsc{dir} \\
-di\#\textsc{erg} \\
-di\#\textsc{obl} \\
-di\#\textsc{sbst} 
\xe

Sometimes multiple glosses are not due to morphological structure, but because the same morph(eme) was given different glosses. For example, interchanging `be' and `is' and `\textsc{cop}' for copular verbs or alternating between lexical glosses (e.g. `you') and grammatical glosses (e.g. `\textsc{2sg.erg}'). Sometimes different glosses appear because the item can be translated by different English words depending on the context. For example, one Lezgi word can be, and is, translated as `be' in some context or 'happen' in others. If alternative labels such as \textit{bahaye\#danger} and \textit{bahaye\#dangerous} are equally frequent, the model must choose randomly. Such inconsistency is to be expected from manual work and could be reduced with more automated assistance from machine learning.

Another pattern of errors is caused by tokens that were only partially segmented (and therefore, not correctly glossed). We knew that many such tokens were included in the gold standard data but there was no reliable way to eliminate them automatically. It is unclear how many exist in each corpus, although Alas and Nat\"ugu seem to have the least. Manipuri [mni] and Lezgi seem to have most incomplete segmentation. This became clear for Manipuri during another project when a language expert was asked to the correct the glosses for several inflected words. It appears that, in the data set, the annotators had been focused only on segmented and glossing certain morphemes on each word, leaving other affixes on the word unsegmented. 
The Lezgi data was annotated by a non-linguist who was trained to use FLEx and did not fully grasp Lezgi's unique morphology or simply did not finish segmenting all words. 

Many quality issues unpredictably increase the number of possible labels and amplify data sparsity. An example is
%did.again.ENT INESS did.again.ENT ENT - INESS is not on verbs
repeated mispelling of glosses (e.g. apperance---appereance---appearance, fourty---forty). Other misspellings originate in transcription. In the Lezgi test data, over 50 misspelled or incorrectly segmented strings were found in the first 200 hundred unique segments, although a few spelling changes are representation of dialectical variations.  

%Lezgi: Some of them should probably not be mulitple:  -bur --\textrangle{} ABS.PL/SBST.PL ==\textrangle{} PL,  -v --\textrangle{} AD/ADESS should be one or the other. 
%Lezgi: The multiple glosses per segment is clearly seen. As our misspellings pr variant spellings in original text:  \foreignlanguage{russian}{вуч вуш} 'what'. 

The results from the Alas corpus were quite good when compared to the much larger corpora. However, the errors are less predictable and more random. It seems likely that the small data set increased the noise to signal ratio and obscured general patterns. 
%The Alas model is less likely to get rare morphemes correct. 
One noticeable confusion was caused by the canonical representation of circumfixes. This is shown in (\ref{ex:alaserrors}) where the model predicted a prefix \textit{n-}. This prefix is a correct surface allomorph of the circumfix at that position. 
%The promiscuous attachment of Alas clitics also seem to difficult to learn.

\pex   
\label{ex:alaserrors}
\a \textbf{GOLD:} \hspace{2mm} n\textlangle{}\textrangle{}ken- \hspace{1mm} nindekh \hspace{1mm} -n\textlangle{}\textrangle{}ken \\
\textbf{OUTPUT:} \hspace{2mm} n- \hspace{5mm} nindekh \hspace{2mm} -n\textlangle{}\textrangle{}ken
\xe

%!Or confusion of stems, which of course are rarer: ke\textlangle{}\textrangle{}en\textrangle{}- kuase -\textlangle{}ke\textlangle{}\textrangle{}en ke\textlangle{}\textrangle{}en\textrangle{}- suasane -\textlangle{}ke\textlangle{}\textrangle{}en
%!Clitics are confusing presumably because they attach to any word in any position: khut -=ne khut

%variation of glosses that should probably be one gloss, perhaps due to context: -\textlangle{}n\textlangle{}\textrangle{}ken#CAUSATIVE -\textlangle{}n\textlangle{}\textrangle{}ken#TO.CAUSE.TO.BECOME..., bahaye#danger bahaye#dangerous

Nevertheless, error analysis shows that the models deal with data sparsity quite well. Even incorrect segments often have very similar character sequences to the correct choice, particularly when the difference is due to a change in the root vowel (e.g. dakhi $\sim$ dikhi). One of the most interesting errors, indicating the model's strong ability to learn patterns even in the face of data sparsity, occurred in Lezgi. The transcribed oral speech has a few dozen codeswitched Russian words. The test data include one or two examples, and in one case the model substituted one codeswitched word with another codeswitched word. 
%Codeswitched: poryadok okruzheniya -е
%\foreignlanguage{russian}{эвецӏун -на эвецӏна -на, хан хун}
%root vowel changes, especially this root 'say' which is quite common in narratives with reported speech:  \foreignlanguage{russian}{лугьун -на лагьа -на}. Also another very common root, 'go' which changes root vowel in difference conjugations:  \foreignlanguage{russian}{хъфена -на хъфин -на}
%Incorrect segmentation that the model corrected:  \foreignlanguage{russian}{i.arada -е i.arada, эхъвена -на экъвен -nа, ya ni ya -ni}
%Or just confused by incorrect segmentation in training data:  пӏузарар -ar -ar пӏузарар -ar -е, gada -di -z gadadiz, gadadin gаdа -di -n
%Confused by semantically motivated allomorphy of ERG/INESS stem:  \foreignlanguage{russian}{къачун -да къачу -е}
%Reduplication?:  \foreignlanguage{russian}{гьагьам гьам, гьа-гьа-гьа гьа-гьа}
%The models Chooses misspelling/variant spelling:  \foreignlanguage{russian}{и кьван икьван}

Many errors noted during error analysis were not actually errors. Since the annotation was originally done by hand, sometimes by multiple annotators, the glosses varied due to misspellings or synonomous glossing choices (e.g. 'BE.PST' vs. 'was'). There was a clear pattern in all datasets for one of the variants to be predicted rather than a random, unrelated label. These cases would not be considered errors by human annotators but were evaluated automatically as errors in the test data. For instance, one Lezgi demonstrative pronoun was sometimes glossed as `these' and sometimes as `this \textsc{-abs.pl}'. In at least once case, the second (and more linguistically precise) analysis was predicted. Unfortunately, because we did not have access to language experts for every corpora, we were not able to normalize our scores based on this knowledge; however, in the future it may be useful to consider that the performance of models trained on field data may, for all practical purposes, be better than the initial scores indicate.

In other cases, the labels in the test data were evaluated as errors, but closer examination revealed that the original human annotation were incorrect in that particular instance and the predicted label was actually the best fit to the data. So, an human error had been ``corrected''. 
avaybur, avariydiz, avatna 2sg.erg - 2sg.abs - ERG.  Some clearly never or inconsistently segmented in training data, or model learns consistent patterns of inconsistent segmenting. Things like  abur -\textrangle{} just inconsistent segmentation/glossing mostly likely. Sometimes right because not always segmented in data (cf. case-stacking). 
Word instances that had been incorrectly segmented by the human annotators were sometimes correctly segmented by the model, although again these examples were evaluated as incorrect because they did not match the gold standard data. For Lezgi, these examples of ``correction'' by the model were more frequent in the sequential system, and may explain why biggest improvement by the sequential system over the joint system is found in the Lezgi data, which we know had many incorrect or incomplete segmentations.  Again, due to the lack of language experts, we are unable to say whether this holds true for all corpora but this should be explored deeper in future research.


\subsection{Feature-Based Results and Discussion}

The results of the CRF and Transformer joint approach were compared as were the results of the CRF+SVM and Transformer+Transformer sequential approach. \autoref{tab:DLFtrResults} shows the difference after subtracting the Transformer results from the feature-based results. Since less variation is expected from the CRF and because the models take significantly longer to train, they were evaluated on a single run, whereas all Transformer results are reported on the average score from a 10-fold cross-validation. 

\begin{table}
    \centering
    \begin{tabular}{l|cc|cc}
        & \multicolumn{2}{c|}{\textbf{Surface}} & \multicolumn{2}{c}{\textbf{Canonical}} \\
        & \textbf{Joint} & \textbf{Seq} & \textbf{Joint} & \textbf{Seq} \\
        \hline
        Alas & .13 & .18 & .06 & .11 \\
        %Alas & .1293 & .1754 & .0626 & .1069 \\
        \hline
        Lamkang &  & .10 &  & .24 \\
        %Lamkang &  & .0985 &  & .2412 \\
        \hline
        Lezgi & .12 & .10 & .15 & .15 \\
        %Lezgi & .1207 & .1028 & .1525 & .1517 \\
        \hline
        Manipuri & .30 & .30 & .15 & .15 \\
        %Manipuri & .3047 & .2996 & .1503 & .1516 \\
        \hline
        Nat\"ugu & .30 & .31 & .23 & .21 \\
        %Nat\"ugu & .2965 & .3132 & .2266 & .2063 \\
        \hline
        Upper Tanana & -.01 & .01 & -.03 & .01 \\
        %Upper Tanana & -.0123 & .0093 & -.0276 & .0084 \\
    \end{tabular}
    \caption[F$_1$-score Differences of Feature-based Models minus Deep Learning]{The differences between the CRF or CRF+SVM models and the Transformer. Positive numbers means the feature-based model outperformed the Transformer.}
    \label{tab:DLFtrResults}
\end{table}

- Difficulty of determining how to align biolabels to word letters in canonical only segmentation. Hence no Tsez feature-based results. Possible future work.

- S only models: The morphs on the segmentation line are not always strictly surface segmented. Those cases were eliminated from the feature-based models since the CRF input and output sequences must be same length. Another thing to consider against deep learning models, which can process sequences of different lengths. Arp: 4163. SKD: 329. Roughly same amount as dev for Transformer training, so not significant in difference. 

- Amount of time required for training CRF. Several days longer in some cases for one run.

- Set up, determining features, writing code, is significantly more complicated with feature-based models. Includes need to workaround non ASCII characters. Better results should be balanced against ease of setting up a deep learning model like the Transformer. 


\section{Conclusion}
\label{sec:sgconclusion}

This paper is aimed at smoothing the road to more interdisciplinary work with NLP and linguistics by articulating and examining the results of different research designs. Different research designs arise from different expectations or conventions in the two fields. Although they do not present barriers to mutually beneficial research, different expectations, such as in segmentation strategies, and different workflows, such as joint or separate segmentation/glossing, should not be dismissed when they arise. This paper tests the possible effects of these two differences.

The small difference between surface and canonical segmentation for three of the five languages suggests either strategy is a useful approach with minimal data, although this changes when data is increased in the joint model. Even though surface segmentation increases the number of labels in a dataset, this appears to be balanced by the by the abstract character of canonical morphemes, most noticeably by circumfixes. The fact that the difference almost disappears when the data size is doubled indicates that the question of segmentation strategy can be eliminated by simply annotating more data with whatever strategy suits the project at hand. 
%This could be done by popularizing segmentation and glossing NLP models in linguistics. 
However, larger differences on Lamkang and Manipuri corpora indicate that the reasons why segmentation strategies does sometimes differ in performance on the same corpsu should be explored more across other Tibeto-Burman languages. Testing the differences in related languages might indicate whether certain linguistics features influence the results of different segmentation strategy when integrating NLP systems. 
%Performance on the same segmentation strategy by different machine learning systems might identify the factors that linguists should consider when choosing machine learning systems. 

The consistent improvement of the sequential system over joint learning may be a reason to consider separating segmentation and glossing tasks in order to leverage the higher accuracy of segmentations, and a more completely segmented corpus, when glossing the corpus. 
%, despite the received wisdom in NLP that joint models tend to outperform separate tasks. 
%However, This may not be practical during the earliest stages of analysis. 
%However, the focus of interlinearization changes once an initial analysis is settled upon; the primary goals become uncovering rare structures and increasing available resources. Machine learning is the best way to do both quickly. Also, 
The strength of the sequential system might be applied when a corpus cannot be completely segmented and glossed due to budget or time constraints. Instead, a strategy would be to prioritize segmenting and benefit from computational assistance when glossing. 
%It also implies that other changes in traditional linguistics workflow may come once NLP models are integrated. 

These studies could serve as a foundation towards more efficient use of computational methods in linguistic analysis and annotation. This paper shows, for example, that the glossing-only model performs well even on inaccurate segmentation predictions and can even ``correct'' manual segmentation errors. The study presented here assumes that the model's segmentation is not corrected by the language experts before training the glossing model. If a human-in-the-loop workflow was introduced to first correct segmentations, then the glossing-only model could improve even more. Such methodological considerations should be tested to see to what extent linguistic analysis and annotation of endangered language might benefit.

%However, the differences between the joint and sequential systems are not great.
%and other factors hint that linguists should adjust their approach to other things than workflows. 
Finally, as \citet{mcmillan-major_automating_2020} noted in glossing research, consistency of the annotations has a strong effect on system performance. This is most clearly seen in Lezgi which is known to be particularly noisy. Random strange characters were found at morpheme boundaries (e.g. {\tt *} instead of  {\tt -}). The human annotators frequently segmented one pair of characters whenever it occurred because it matched a frequent suffix. Allomorphs were frequently glossed as if they were different morphemes, undoing the benefit of canonical segmentation. Finally, its unique case-stacking caused confusion both to the human annotator and to the system results, in particular because one morpheme with several semantically-motivated allomorphs is (incorrectly) glossed one way when it stands as a single case marker and glossed another way when it precedes additional case markers.
%LEzgi: Missing intermedial affixes in case stacking:  imi -di -ni imni
%Confusing OBL and ERG. They are identical but OBL is tagged in case stacking (fairly consistently). 

So what would happen if linguists emphasized quality over quantity? We can answer this question by comparing Lezgi to Alas. According to the accounts of the linguists involved, and evidenced by our experimental results, the Alas data was annotated much more consistently and meticulously. With a corpus one third the size of the Lezgi corpus, the Alas model performs almost equally well. It is possible but seems unlikely that this is due to differing morphological structure. Unlike Lezgi---which is overwhelmingly suffixing and has fairly limited morphophonological changes---Alas features prefixing, suffixing, circumfixing, and infixing with various morphophonological processes. The main difficulty for the Alas systems was the sparsity of stems, compared to oft-repeated affixes. 

Interestingly, Alas showed the least marked preference between sequential and joint learning. This may indicate that higher consistency may eliminate the need to consider any change to segmentation/glossing workflow, but it should be investigated with further experiments focused on differences in annotation quality. Preferably these experiments would conducted on closely related languages to reduce effects due to different typology. 

%A downside to CRF in particular is that it takes so long to train. LMK and ARP run for over 200 hours with finishing. Whatever advantage may come from feature-based models has to be weighed against difficulty of formatting data and feature engineering. Lezgi and Lamkang took significantly longer to train on CRF. Because of Noise?


When considering low-resource settings, consistency for machine learning seems more important than data size, strategy, or workflow. Ruthless consistency is not something linguists have had reason to put high value on and it is not something to be expected by manual annotation, %although copying features and rule-based parsers in current software tools are helping.\mans{Copying? I don't follow this sentence.} 
Consistency can be provided by machine learning integration, but ironically, supervised machine learning needs high consistency in annotated data before it can perform accurately enough to assist human annotators by increasing their speed or accuracy. Our best estimate of the accuracy threshold for practical integration of machine learning into annotation is 60\% \citep{felt_improving_2012}. This threshold on F$_1$-scores was soundly passed by Lamkang because it over 18k manually annotated tokens for training but it was barely reached by the corpora with 4.5k-5.5k tokens. However, the meticulously annotated Alas corpus came close to this threshold with only 1.5k training tokens. If linguists wish to successfully integrate machine learning into the documentation and description of under-documented and endangered languages, then they must adopt from NLP an emphasis on highly consistent annotation.

%\section{Conclusion}

% The results indicate whether linguists should consider adjusting their work to NLP assumptions in order to optimize their efforts. Or whether NLP should consider the reality of working with linguistic resources. If they can expect that IGT data is both segmented and glossed, and not segmented without glosses, then they might want to adjust their experiments to this reality. Now, if a sequential method of first segmenting and then glossing gets better results, then the NLP experiments may take jointly segmented and glossed data and still choose sequential training to produce both. But if joint segmentation and glossing is a better way to leverage low-resource IGT data, then NLP researchers should consider re-framing their experiments in low-resource languages. NLP research should take into consideration real-world situation. Low-resource data is rarely going to be curated and carefully preprocess. There is not a lot of options if the data is not what was expected. NLP needs to be flexible and adapt to the noisy realities.

%NLP has made it possible to achieve more with less. Methods for low-resource languages are improving. These methods are slowing being introduced to linguistics. Some have been applied to language documentation tasks. However, these experiments make assumptions that are not based on a understanding of linguistics methods.
%Building systems that integrate well with linguistic conventions while also feeding NLP development effectively will undoubtedly require adjustments on both sides, but we should try to minimize those adjustments and optimize our work so that more languages can be documented/described and more communities can have equitable access to language technology. 

%Linguists segment and gloss a word before segmenting another next word.
%Supervised NLP systems have assumed the two tasks are completed independently. If NLP systems perform better by completing these task separately, then linguists may get optimal benefit by following this workflow. However, it would be better if NLP systems could adjust to the linguist's traditional workflow, if possible. If joint segmentation and glossing performs as well or better than a sequential model, then joint models should be preferred. 

%Linguists prefer canonical segmentation, but may surface segmentation is also used, and may be more common in data processed by Toolbox. 
%NLP systems will either prefer one, in which case linguists may need to adjust their workflow (e.g. do surface segmentation and glossing first, finish the texts with automated assistance, then return to do canonical segmentation, hopefully automated and presumably with the assistance from completed surface segmentation and glosses), or NLP do prefer one over the other, in which case linguists can integrate segmentation models into their work without adjustment. 

%A key to NLP success in low-resource settings is making the most of whatever resources do exist. It is important that automated segmentation/glossing systems integrate easily into linguists' workflow because, to achieve high accuracy (over 90\%) on limited data, NLP must eventually bring humans back into the loop.
%Integrating NLP into language documentation will mostly involve active learning. As Palmer et al. (??) showed, active learning must take the annotator into account to be effective. NLP methods should take the annotator/linguist into account in order for those methods to be optimally effective in the long-term. Automated work does not have this constraint if it assumes no active learning scenario. But that limits its usefulness to language documentation. With limited data, machine learning models cannot be expected to achieve very high accuracy. Its mistakes will have to be corrected by an annotator and and efficient combination of manual work and computer algorithm is to leverage those corrections to quickly improve the model with some sort of human-in-the-loop method, such as active learning.