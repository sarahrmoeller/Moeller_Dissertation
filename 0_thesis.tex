\documentclass[defaultstyle,11pt]{thesis}

\usepackage{amssymb}		% to get all AMS symbols
\usepackage{graphicx}		% to insert figures
\usepackage{hyperref}		% PDF hyperreferences??

\usepackage{float}
\usepackage{enumitem}
\usepackage{expex}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.1}
\usepackage{natbib} % to use Coling style citations
\usepackage{comment}

%\usepackage{epstopdf}
%\usepackage[koi8-ru,latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage[T2A,T1]{fontenc}
%\usepackage[russian,english]{babel}


%%%%%%%%%%%%%%% TO DO NOTES %%%%%%%%%%%%%%%%%%%%%%%
\usepackage{todonotes}

\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} % default note settings, used by macros below.
\newcommand{\mans}[2][]{\note[#1]{mans}{lightblue!40}{#2}}
\newcommand{\Mans}[2][]{\mans[inline,#1]{#2}\noindentaftertodo}


%%%%%%%%%%%%   All the preamble material:   %%%%%%%%%%%%

\title{Integrating Machine Learning into Language Documentation and Description}

\author{Sarah}{Moeller}

\otherdegrees{B.A., Thomas Edison State College, 2002 \\
	      M.A., Dallas International University, 2010 \\
	      M.A., University of Colorado, 2020}

\degree{Doctor of Philosophy}		%  #1 {long descr.}
	{Ph.D., Linguistics and Cognitive Science}		%  #2 {short descr.}

\dept{Department of}			%  #1 {designation}
	{Linguistics and Institute of Cognitive Science}		%  #2 {name}


\advisor{Dr.}				%  #1 {title}
	{Mans Hulden}			%  #2 {name}

\reader{Dr. Martha Palmer}		%  2nd person to sign thesis
\readerThree{Dr. Andrew Cowell}		%  3rd person to sign thesis

%\readerFour{Dr. Alexis Palmer}	

%\readerFive{Dr. Katharina Kann}	

\abstract{  \OnePageChapter	% because it is very short

Any attempt to integrate Natural Language Processing (NLP) machine learning systems to the study of languages must take into consideration approaches and expectations of both NLP and linguistics. This dissertation examines various strategies and workflows that may affect the potential to integrate machine learning in the documentation and description of endangered languages. Unlike previous studies, it applies machine learning to multiple corpora of typologically diverse endangered languages. 


First, it examines automating segmentation and glossing as a way to remove the ``annotation bottleneck'' that plagues documentary and descriptive linguistics. Three experiments train Transformer models on interlinearized corpora from five under-documented languages. The first experiment compares segmentation and glossing as a joint step to approaching the tasks as two sequential steps. We find the sequential approach yields somewhat better results.
%, indicating that linguist should perhaps adjust their traditional approach. 
The second experiment looks at the outcome when training on surface-segmented data, where strings of texts have been simply divided at morpheme boundaries as is common in NLP, and compares it to same corpora trained on canonically segmented data, the segmentation strategy preferred by linguists where abstract, underlying forms are represented. No clear advantage to either segmentation strategy is found and the difference between them disappears as the training data increases. On average the models achieve more than a 0.5 F$_1$-score, with the best models scoring 0.6 or above. An analysis of errors indicates that consistency during manual segmentation and glossing may facilitate higher scores from NLP models, and, although scores evaluated against  instances of annotator error in the original data are ``corrected'' by the model.


An intermediate step in the linguistic analysis of an under-documented language is to discover morphological inflectional patterns. The first step towards this goal can be to induces patterns from the inflected forms that are attested in transcribed texts.  
From this data, new inflected word forms can be created in order to test hypotheses about the language's inflectional patterns and to begin to construct inflectional paradigm tables. This dissertation presents a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). 
IGT2P generates entire morphological paradigms from IGT input.
The study shows that existing morphological reinflection models can solve the task with $21\%$ to $64\%$ accuracy, depending on the language. 
It also finds that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.


Part-of-Speech (POS) tags routinely appear as features in morphological tasks. POS taggers are often one of the first NLP tools developed for low-resource languages. However, as NLP expands to new languages it cannot assume that POS tags will be available to train a POS tagger. This paper empirically examines the impact of POS tags on two morphological tasks with the Transformer architecture. Each task is run twice, once with and once without POS tags, on otherwise identical data from ten well-described languages and five under-documented languages. We find that the presence or absence of POS tags does not have a significant bearing on the performance of either task. In joint segmentation and glossing, the largest average difference is an .09 improvement in F$_1$-scores by removing POS tags. In reinflection, the greatest average difference is 1.2\% in accuracy for published data and 5\% for unpublished data. These results are indicators that NLP may benefit from documentary linguistic data even when a POS tag set does exist for a language yet, and \textit{vice versa}.
  
}

\dedication[Dedication]{	% NEVER use \OnePageChapter here.
		To my parents who have a habit of replying, ``Go for it!"
	}

\acknowledgements{	%\OnePageChapter	% *MUST* BE ONLY ONE PAGE!
    
    This dissertation was supported by generous grants from the Institute of Cognitive Science (ICS), the Center for the Advancement of Teaching in Social Sciences (CARTSS), the Departmet of Linguistics at the University of Colorado Boulder, and by International Language and Development (ILAD). I am very grateful to Daniel Wilson who connected me with ILAD--a wonderful and innovative organization.
    
    I could not have done this without the valuable guidance, consistent support, and encouragement of my advisor Mans Hulden. Despite his workload, he never hurries through our meetings and he contributed to my success even beyond the helpful discussions about research. As an advisor he gave an ideal balance of expectations and independence. 
    
    My committee members all contributed more to this completed manuscript than just comments and suggestions. Martha Palmer
    %provided several semesters of fellowships, insights from her broad experience, and down-to-earth encouragement. She 
    gave simple, practical advice that, especially during my first year, made this journey easier. Andy's keen interest in computational methods for minority languages has been a great example of cross-disciplinary research and lifelong learning. Alexis has shown the spirit of a true teacher and mentor since we first crossed paths in Portugal and has been generous with her time and with my odd question about careers in academia. Without Katharina's insightful questions and pursuit of excellence, I'm not sure the IGT2P and POS chapters would have come to existence.
		
    The quality of this work owes much to the other co-authors of the published versions of Chapters \ref{chap:IGT2P} and \ref{chap:POS}. Ling Liu was ready to listen to the vague idea and take on another project in the middle of her own PhD progress. I'm so glad Changbing Yang asked for more opportunities and I wish her the best as she starts her PhD. 
    
    Without the linguists, fieldworkers, and annotators who prepared the IGT, this work would not have been possible. Drs. Brenda Boerger, Shobhana Chelliah, Bernard Comrie, and Andy Cowell, as well as Chuck Donet, and Andrew Brumleve generously shared their field data. Yaghut added the missing Lezgi annotations. Mary Burke, Brenda, Andy, and Andrew also did the expert cleaning for the IGT2P experiments. Changbing and my undergraduate research assistants Zachary J. Ryan and Huilin Lin helped preprocess and reformat much of the data.
    
    I am so grateful for colleagues at CU Boulder who made time to talk about life and linguistics: Kristin, Katie, Irina, Annebeth, Adam, and of course my ever encouraging and upbeat cohort-mate Norielle. 
    
    Perhaps the biggest reasons I considered a career in academia were the examples set by Paul Kroeger, my MA advisor, and Michael Boutin, the head of the Applied Linguistics Department at Dallas International University. I didn't write a MA thesis, so this is my chance to acknowledge them. 
    %It wasn't just their academic guidance that inspired me. 
    My goal is to give others what they gave to me: the sense that every time I entered their offices that they had nothing more important in their overworked life than to listen to my rambling ideas and answer my half-formed questions. 
    
    Several other people from my Texas years encouraged me to head down this road. Among them, Brenda Boerger and Will Reiman are wonderful mentors and faithful friends whose Skype pings have meant so much the past several years.
    
	More than anyone, I am grateful for the support of my my family. They actually seem convinced everything I do is interesting and important. And I am obliged to my 16 nephews and nieces who kept me grounded by demanding, %``You are weird!" and 
	``If you're almost-a-doctor, how come you don't know the answer to my question?!"
	}

% \IRBprotocol{E927F29.001X}	% optional!

\ToCisShort	% use this only for 1-page Table of Contents

\LoFisShort	% use this only for 1-page Table of Figures
% \emptyLoF	% use this if there is no List of Figures

\LoTisShort	% use this only for 1-page Table of Tables
% \emptyLoT	% use this if there is no List of Tables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%       BEGIN DOCUMENT...         %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\input macros.tex
\input 1-Intro.tex
\input 2-Litreview.tex
\input 3-DataModels.tex
\input 4-SegGloss.tex
\input 5-IGT2P.tex
\input 6-POS.tex
\input 7-Conclusion.tex

%%%%%%%%%   then the Bibliography, if any   %%%%%%%%%
%\bibliographystyle{plain}	% or "siam", or "alpha", etc.
%\nocite{*}		% list all refs in database, cited or not
%\bibliography{refs}		% Bib database in "refs.bib"

%%%%%%%%%%%%%%%% COLING STYLE %%%%%%%%%%%%%%%%%%%
\bibliographystyle{acl_natbib_nourl}
\bibliography{refs,SegGloss,IGT2P,POS}

%%%%%%%%%   then the Appendices, if any   %%%%%%%%%
\appendix
\input appendixA.tex
%\input appendixB.tex

\end{document}

