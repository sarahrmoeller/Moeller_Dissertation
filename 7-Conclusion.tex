\chapter{Conclusion}
\label{chap:conclusion}

This work investigates the potential to advance the science of linguistics through the integration of NLP machine learning systems. This research has broad intellectual merit. It provides more usable data in several underdocumented languages, leverages field data for NLP, and test machine learning model efficacy with limited and noisy data---often the only data available for endangered or low-resource languages. It also explores murky issues surrounding conventional expectations that derived from different approaches in NLP and in linguistics.
%nature and role of syntactic categories with an empirical study.

The outcomes described here have impact on future direction of language documentation and description. Linguists who wish to benefit from automated assistance in these tasks may want to consider how to adjust their research design and NLP scientists who wish to expand into low-resource languages may want to consider what kind of data they can expect when working with available data. As much as the amount of data, annotation quality also determines how machine learning models perform.

The implications of this research has broader impact in several directions. First, the integration of machine learning for learning morphological paradigms indicates that NLP tools and methods can be used to build and test hypotheses during structural analysis of a previously underdescribed language. This potential to use machine learning to advance linguistic analysis, in turn, has implications for refining and testing linguistic theory on new languages. Second, although this work focuses mainly on issues and needs related to endangered languages, even very stable languages such as Manipuri [mni] with a million or more speakers may have limited annotated resources available to train NLP models on. The methods presented in this research can be applied to any low-resource language, and by extension, to low-resource genres or domains in high-resource languages. Third, the growth of linguistic data and analytical knowledge that machine learning can give for underdocumented and underdescribed languages may benefit communities who wish to develop language maintenance or revitalization material. 


%If documentary and descriptive IGT that has been manually annotated can be utilized for computational induction of morphological inflection patterns with minimal need for domain experts, then both linguists and NLP practitioners can confidently increase their collaboration. They can further develop NLP for low-resource settings and perhaps focus together on systems that mutually benefits the discipline of linguistics, the development of NLP, and the practical needs of communities engaged in language revitalization or maintenance. 

%If the presence of POS tags makes a significant difference in the results of other automated tasks that are helpful to the documentary and descriptive workflow, then linguists may want to re-examine their accepted workflow and emphasize early analysis and labeling of syntactic categories. If the difference is trivial, then linguists have little reason to allocate early time and funding to POS tagging despite its importance to NLP. NLP practitioners, in turn, may need to reconsider their reliance on POS tags for some tasks. 

%%%%%%%FUTURE%%%%%%
The three general studies presented in the previous three chapters discussed future work specific to each study but the natural future directions of this work as a whole should be considered. All three studies are presented as static experiments, where machine learning is trained on manually annotated data and the model's performance is measured by a single cycle of training and testing. This static approach gives the impression that the real-life application would be limited to spitting out results of a certain accuracy which linguists can accept as is without further interaction or consideration of machine learing integration or the possibility of continued automated assistance. If accurate analysis and annotation is to be completed, then the model's output must be manually vetted and corrected. This situation should not be the end of the story. 

A natural future direction is to build iterative integration of machine learning into the continued work of language documentation and description. This should be done at all stages of the workflow, not just morphological analysis and annotation, but I will outline here how it could be done with the tasks explored in this dissertation. 

Since linguistics degree still rarely include computer programming skills, the practical real-life end goal is a computer software interface that integrates machine learning and works for any language. 

Machine learning should be integrated into language documentation and description in a human-in-the-loop or active learning cycle. With segmentation and glossing, this would look like this. After initial human annotation a model would be trained with the most effective methods, such as the ones discussed in this dissertation. Before It should then strategically choose small batches (~50) of tokens. Present context to annotator and have them correct or vet them. Then retrain. The question is how to select those batches so that the model achieves highest accuracy in shortest time. 

Three common active learning strategies are random, uncertainty, and sequential. Compare selecting with linguistic factors and/or selecting with weighting by lingustic factors. General linguistic factors: length of word, similar substrings on words with high uncertainty (assume these indicate morphological forms not yet learned well), POS that have multiple glosses (assumed to indicate morphological complex POS), ...
Language- or areal- or genetic-specific factors: POS that is known to be most complex (e.g. verbs in Upper Tanana), position of word in sentence, specific substring (indicating inflectional class or derivational complications), ...

Compare these strategies using pre-annotated data as stand-in for human annotations. Run one time to see immediate improvement. Run multiple time to see improvement over several iterations. Compare single strategies. Then combine different strategies and compare. Try adding strategies at different points in iterative progress to identify if a strategy is more helpful at certain point. 

Next run with human annotators to see how the strategies interact. Measure both time and accuracy. Measure workhours to achieve preset accuracy goal. Measure average time per annotation and how this changes through iterations. Consider linguistic expertise/familiarity of annotators. 

Build software. Collaborate with HCI and UX. 

Test software and previous factors in actual documentary/descriptive field project. Test the whole process.

\#\#\#

A natural future direction is to address the practical implications of integrating machine learning. One implication is the potential for ``active learning'' where machine learning models are iteratively re-trained with a small set of new manual annotations. A key issue is how to select data for the new manual annotations so that the model will improve very quickly with least manual effort. Selection methods based on computational and linguistic motivations need to be tested and compared. For example, active learning for paradigm induction would involve first training on partial tables, then comparing results after retraining on data selected by different methods: 1) selecting word forms from the modelâ€™s predictions that it had least ``confidence'' about, 2) randomly selecting word forms, 3) selecting a random number word forms but making sure they are evenly distributed among tables, or 4) selecting word forms so that each table has the same minimum number of forms per table. This same experiment could be conducted for joint segmentation and glossing. The first two methods would remain the same, but other methods could be 3) selecting a number of least frequent word types, 4) selecting word forms based on statistically rare letter sequence combinations (in attempt to identify rare morphological combinations), or some other method motivated by linguistic facts of the language such as 5) annotating only nouns in Lezgi which are most likely to have irregular allomorphy. Either of these experiments are potential additional or alternative studies to the research that is described in this Prospectus.
