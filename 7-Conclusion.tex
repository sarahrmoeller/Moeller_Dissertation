\chapter{Conclusion}
\label{chap:conclusion}

This work investigates the potential to advance the science of linguistics through the integration of NLP machine learning systems. It shows that effective machine learning methods can be integrated into morpheme segmentation and glossing and into morphological paradigm learning. Effective methods are shown to be those that do not depend on conventional expectations or traditional research designs of linguistics or NLP, including expectations of surface or canonical segmentation, joint or sequential approaches to segmentation and glossing, assumptions about the superiority of deep learning models, the expense of human annotation, or the necessity of POS tags. 

This research has broad intellectual merit. It provides more usable data in several underdocumented languages, leverages field data for NLP, and test machine learning model efficacy with limited and noisy data---often the only data available for endangered or low-resource languages. It also explores murky issues surrounding conventional expectations that derived from different approaches in NLP and in linguistics.
%nature and role of syntactic categories with an empirical study.

The outcomes described here have impact on future direction of language documentation and description. Linguists who wish to benefit from automated assistance in these tasks may want to consider how to adjust their research design and NLP scientists who wish to expand into low-resource languages may want to consider what kind of data they can expect when working with available data. As much as the amount of data, annotation quality also determines how machine learning models perform.

The implications of this research has broader impact in several directions. First, the integration of machine learning for learning morphological paradigms indicates that NLP tools and methods can be used to build and test hypotheses during structural analysis of a previously underdescribed language. This potential to use machine learning to advance linguistic analysis, in turn, has implications for refining and testing linguistic theory on new languages. Second, although this work focuses mainly on issues and needs related to endangered languages, even very stable languages such as Manipuri [mni] with a million or more speakers may have limited annotated resources available to train NLP models on. The methods presented in this research can be applied to any low-resource language, and by extension, to low-resource genres or domains in high-resource languages. Third, the growth of linguistic data and analytical knowledge that machine learning can give for underdocumented and underdescribed languages may benefit communities who wish to develop language maintenance or revitalization material. 


%If documentary and descriptive IGT that has been manually annotated can be utilized for computational induction of morphological inflection patterns with minimal need for domain experts, then both linguists and NLP practitioners can confidently increase their collaboration. They can further develop NLP for low-resource settings and perhaps focus together on systems that mutually benefits the discipline of linguistics, the development of NLP, and the practical needs of communities engaged in language revitalization or maintenance. 

%If the presence of POS tags makes a significant difference in the results of other automated tasks that are helpful to the documentary and descriptive workflow, then linguists may want to re-examine their accepted workflow and emphasize early analysis and labeling of syntactic categories. If the difference is trivial, then linguists have little reason to allocate early time and funding to POS tagging despite its importance to NLP. NLP practitioners, in turn, may need to reconsider their reliance on POS tags for some tasks. 

%%%%%%%FUTURE%%%%%%
The three general studies presented in the previous three chapters discussed future work specific to each study but the natural future directions of this work as a whole should be considered. All three studies are presented as static experiments, where machine learning is trained on manually annotated data and the model's performance is measured by a single cycle of training and testing. This static approach gives the impression that the real-life application would be limited to spitting out results of a certain accuracy which linguists can accept as is without further interaction or consideration of machine learning integration or the possibility of continued automated assistance. If accurate analysis and annotation is to be completed, then the model's output must be manually vetted and corrected. This situation should not be the end of the story. 

A natural future direction would address the practical implications of machine learning for documentary and descriptive linguistics by integrating an iterative cycle of machine learning into ongoing work.  One potential is active machine learning where models are iteratively re-trained on small sets of new annotations. A key issue is how to select data for the new manual annotations so that the model will improve very quickly with least manual effort. Selection methods based on computational and linguistic motivations need to be tested and compared. 
%Machine learning should be integrated into language documentation and description in a human-in-the-loop or active learning cycle. 
A first model could be trained on initial human annotation with the most effective methods as described in this dissertation, but instead of simply returning the model's predictions to the annotators for correcting, a human-in-the-loop approach would be introduces. Another machine learning system would learn to strategically select small ($\sim$50-100) batches of tokens from the predictions. These would be presented with context to annotator for correcting or vetting. The first model would be then retrained. The key question is how to select the additional tokens that are most informative, so that with small batches of annotation the model improves exponentially. 

Selecting the most informative tokens for additional annotation is a task for active machine learning. The common active learning selection strategies are random, uncertainty, and sequential. These do not integrating linguistic knowledge. With limited data, we need to make the most of all available resources, including human resources. Linguists who have begun to analyze a language have knowledge about even an undescribed languages, as do native speakers although they may not be trained linguists. The active learning stage should integrate knowledge about the language, from humans and any published description, into selection strategies. For example, a pilot study could compare the generic selection strategies against strategies based on linguistic factors, such as selecting longer words, selecting words with substrings found on words that the machine has high uncertainty 
%(assume these indicate morphological forms not yet learned well), 
selecting identical words with multiple (ambiguous or inconsistent) glosses 
%(assumed to indicate morphological complex POS),
or POS tags, selecting lexical categories that are known to have complex morphology
%(e.g. verbs in Upper Tanana), 
selecting based on word order, selecting words with specific substrings that may indicate inflectional classes or derivational complications, etc. 

This should be done at all stages of the workflow, not just morphological analysis and annotation, but I will outline here how this might be done with IGT2P.
Active learning would involve first training on partial tables, then comparing results after retraining on data selected by different methods: 1) selecting word forms from the modelâ€™s predictions that it had least ``confidence'' about, 2) randomly selecting word forms, 3) selecting a random number word forms but making sure they are evenly distributed among tables, or 4) selecting word forms so that each table has the same minimum number of forms per table. This same experiment could be conducted for joint segmentation and glossing. The first two methods would remain the same, but other methods could be 3) selecting a number of least frequent word types, 4) selecting word forms based on statistically rare letter sequence combinations (in attempt to identify rare morphological combinations), or some other method motivated by linguistic facts of the language such as 5) annotating only nouns in Lezgi which are most likely to have irregular allomorphy. 

Later studies could choose the most informative factors and use them to weight selection with the generic random, uncertainty, or sequential selection strategies.
Each strategy could be compared on immediate improvement after one batch of annotation and then after multiple runs in order to compare the performance curve over several iterations. 

%Compare single strategies. Then combine different strategies and compare. Try adding strategies at different points in iterative progress to identify if a strategy is more helpful at certain point. 

Eventually, the active learning, or human-in-the-loop experimentation, will of course need to involve actual human annotation, but since that introduces complications related to levels of expertise, and various speeds of annotation, the first experiments should use pre-annotated data as stand-in for the iterative human annotation. Once the most effective strategy, or combination of strategies, is found human annotation should be introduced to study how the strategies interact with human pace and (in)accuracy. 
%Measure workhours to achieve preset accuracy goal. Measure average time per annotation and how this changes through iterations. Consider linguistic expertise/familiarity of annotators. 

Finally, human-in-the-loop approach should be tested on the field. 
Since linguistics degree still rarely include computer programming skills, the practical real-life end goal is a computer software interface that integrates machine learning and works for any language. This would require building a software interface which could motivate the project to become even more interdisciplinary and collaborative. It could be an excellent chance to introduce experts in Human Computer Interaction or User Experience design who could build and test an interface for fieldwork where some users may have very low computer literacy.

%Test software and previous factors in actual documentary/descriptive field project. Test the whole process.