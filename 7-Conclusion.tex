\chapter{Conclusion}
\label{chap:conclusion}

The need to address language endangerment by increasing documentary and descriptive data is great. Integrating machine learning into the process is perhaps our best hope of addressing this with reasonable speed and accuracy. Systems that are reasonably effective in limited data settings already exist, as do archived corpora of documentary and descriptive data that could be used to train those systems. However, the two have barely been leveraged to benefit work in NLP and linguistics. That is why this dissertation explored how such integration might affect current workflows and lead to new methods. 

In this dissertation, I examined methods for training machine learning systems on limited data and integrating those systems into the documentation and description of endangered languages, with a specific focus on morphological analysis. Unlike previous studies, I involved more than one or two languages, experimenting with corpora from nine typologically diverse languages. The research results can be described as follows. First, various methods for automating morpheme segmentation and glossing were compared. Treating the two tasks as sequential steps, where a separate machine learning model is trained on each step, does slightly better at both tasks than treating segmentation and glossing as a single, joint step. This indicates that linguists who integrate machine learning may want to plan their project to focus first on segmentation, and after segmentation, to use an accurate segmentation model to assist with glossing. 
Whether a segmentation model should be trained on surface segmentation rather than canonical, or underlying, segmentation is unclear, and what difference there is between the two segmentation strategies nearly disappears when the amount of training data is doubled. 
When comparing feature-based and deep learning  models, it turns out that the older feature-based models do better or at least as well as the newer, state-of-the-art deep learning models. Feature-based models outperform deep learning models by as much as .3 F$_1$-score.
Second, a new task was presented for learning morphological paradigms and automatically generating new morphological resources, called: IGT-to-paradigms (IGT2P). Human annotation was integrated into IGT2P and demonstrated a small amount of additional annotation can increase the model's accuracy by $2\%$ to $16\%$. This proves that machine learning can be used not only for repetitive annotation tasks but also for more complex descriptive work. 
Third, the presence or absence of POS tags when performing joint segmentation+glossing and paradigm induction was examined and shown to have little impact on either task. The largest average difference across six languages was a mere .09 improvement in F$_1$-scores by removing POS tags. This indicates that NLP may benefit from documentary linguistic data even when customarily expected kinds of annotation do not yet exist.

The results have implications in several areas. The research provided more usable data in several under-documented languages, leveraged field data for NLP, and tested machine learning model efficacy with limited and noisy data---often the only data available for endangered or low-resource languages. It explored murky issues derived from different approaches in NLP and in linguistics. 
The integration of machine learning for learning morphological paradigms indicates that NLP tools and methods can be used to build and test hypotheses during structural analysis of a previously undescribed language. The potential to use machine learning to advance linguistic analysis in more languages, in turn, could support the refining of linguistic theories. Furthermore, the methods explored in this research can be applied to any low-resource language, and by extension, to low-resource genres or domains in high-resource languages. Although I have focused mainly on the implications for endangered languages, even very stable languages such as Manipuri [mni] with a million or more speakers have limited annotated resources available and the increase of linguistic data and analytical knowledge will support NLP expansion into those languages and could benefit communities who wish to create language learning or literacy materials. 

%%%%%%%FUTURE%%%%%%
The three studies described in the previous three chapters discussed future work specific to each study but this work as a whole has natural future directions that should be considered. All three studies are presented as static experiments, where machine learning is trained on manually annotated data and the model's performance is measured by a single cycle of training and testing. This static approach gives the impression that the real-life application would be limited to spitting out results of a certain accuracy which linguists can accept as is without further interaction or consideration of machine learning integration or the possibility of continued automated assistance. If accurate analysis and annotation is to be completed, then the model's output must be manually vetted and corrected. This situation should not be the end of the story. 

The practical implications of integrating machine learning into documentary and descriptive linguistics should be explored by a more realistic iterative cycle of machine learning and manual annotation. Such a human-in-the-loop cycle, sometimes known as active learning, re-trains models on small sets of new manual annotations. 
For example, a first model could be trained on initial human annotation with the most effective methods as described in this dissertation, but instead of simply returning the model's predictions to the annotators for correcting, a human-in-the-loop approach would be introduced. Another machine learning system would learn to strategically select small ($\sim$50-100) batches of tokens from the predictions. These would be presented with context to the annotator for correcting or vetting. The first model would be then retrained. The key question is how to select the additional tokens that are most informative, so that with small batches of annotation the model improves as fast as possible. 

A key question is how to select the data to be annotated so that the model will improve very quickly with least manual effort. Selection methods based on computational and linguistic motivations need to be tested and compared. 
Common active learning selection strategies do not integrate linguistic knowledge. %With limited data, we need to make the most of all available resources, including human resources. 
Linguists who have begun to analyze a language have gained linguistic knowledge about the languages. Human-in-the-loop experimentation should integrate this knowledge into the strategies for selecting the most informative tokens to be annotated before re-training. For example, a study might compare generic selection strategies such as uncertainty sampling against strategies based on linguistic factors, such as selecting longer words (assuming they are more morphologically complex and therefore more informative), and then compare against a combination of methods, such as selecting words with substrings found on words about which the machine prediction has high uncertainty.
%(assume these indicate morphological forms not yet learned well), 
%selecting identical words with multiple (ambiguous or inconsistent) glosses 
%(assumed to indicate morphological complex POS),
%or POS tags, selecting lexical categories that are known to have complex morphology
%(e.g. verbs in Upper Tanana), 
%selecting based on word order, selecting words with specific substrings that may indicate inflectional classes or derivational complications, etc. 

This should be done at all stages of the workflow, but I will outline here how it might be done with IGT2P.
Active learning would involve first training a model on partially completed inflectional paradigm tables that were filled from forms attested in documentary data, as described in Chapter \ref{chap:IGT2P}, then comparing results after retraining the model once on data selected by different methods. These methods could be: 1) selecting word forms from the model’s predictions that it had least ``confidence'' about, 2) randomly selecting word forms, 3) selecting a random number of word forms but making sure they are evenly distributed among tables, or 4) selecting word forms so that each table has the same minimum number of forms per table. This same experiment could be conducted for joint segmentation and glossing. The first two methods would remain the same, but other methods could be 3) selecting a number of least frequent word types, 4) selecting word forms based on statistically rare letter sequence combinations (in an attempt to identify rare morphological combinations), or some other method motivated by linguistic facts of the language such as 5) annotating only nouns in Lezgi which are most likely to have irregular allomorphy. Each strategy could be compared on immediate improvement after one batch of annotation and then after multiple runs in order to determine short-term and long-term effect on the performance curve. 
%the most effective factors could be chosen and combined with selection based on random, uncertainty, or sequential selection of tokens.

%Compare single strategies. Then combine different strategies and compare. Try adding strategies at different points in iterative progress to identify if a strategy is more helpful at certain point. 

Eventually, the active learning, or human-in-the-loop experimentation, will of course need to involve actual human annotation. However, that introduces complications related to levels of expertise, and various speeds of annotation, and expense of hiring annotators. Therefore, the first experiments should use pre-annotated data as a stand-in for iterative human annotation. Once the most effective strategy, or combination of strategies, is found, then human annotation should be introduced to study the effect of human pace and (in)accuracy. 
%Measure workhours to achieve preset accuracy goal. Measure average time per annotation and how this changes through iterations. Consider linguistic expertise/familiarity of annotators. 

Finally, a human-in-the-loop approach should be tested \textit{in situ}, as it would be used in a documentary and descriptive project, perhaps during fieldwork, so that its effect on the workflow of a field project can be observed and the methods adjusted as practical demands may require. 
Since linguistics degree programs still rarely include computer programming skills, this would require a usable software interface. This requirement should motivate more interdisciplinary and collaborative work. It could be an excellent chance to introduce experts in Human Computer Interaction or User Experience design who could build and test an interface for fieldwork.

%Test software and previous factors in actual documentary/descriptive field project. Test the whole process.
%%%%%%%%%%%%%%%
This work could impact the workflow of language documentation and description. Linguists who wish to benefit from automated assistance may want to consider how to adjust their research design. NLP scientists who wish to expand into low-resource languages may want to consider what kind of data they can expect when working with available data. Annotation quality determines how machine learning models perform, perhaps as much as the amount of training data.

In conclusion, this dissertation has demonstrated the potential to advance the science of linguistics through the integration of NLP machine learning systems. It has shown that effective machine learning methods can be integrated into morpheme segmentation and glossing and into morphological paradigm learning. Effective methods are shown to be those that do not depend on conventional expectations or traditional research designs of linguistics or NLP, including expectations of surface or canonical segmentation, joint or sequential approaches to segmentation and glossing, assumptions about the superiority of deep learning models, the expense of human annotation, or the necessity of POS tags. 


%One goal of the research is to prove that machine learning systems can achieve reasonable performance when trained on the often noisy data produced by documentary and descriptive projects.

%study effective integration of computational methods that could open the annotation bottleneck. 
%caused by current time-consuming manual methods to produce morpheme segmentation, glossing, free translation, and first-pass hypotheses of morphological inflectional paradigms. 

%First, with successful automated joint segmentation and glossing it will integrate machine learning into the documentary and descriptive pipeline for several typologically different languages. This will demonstrate the potential for producing new annotated data more quickly and accurately than is possible with current methods. While other research has demonstrate this with systems for one or two languages, this research will be conducted on several languages and will show the practicality of automated assistance for morphological annotation regardless of language typology. 

%Second, by testing a process to learn inflectional paradigms from field data the proposed dissertation will demonstrate the potential for machine learning to assist linguists in deeper analysis and description. This is a step towards integrating machine learning assistance beyond early documentary stages and using it to build and test holistic hypotheses about a language's structure.

%Third, the dissertation will unite the efforts of linguistics and NLP by successfully training machine learning on linguistic field data. A main theme of the proposed research is that machine learning systems can achieve reasonable performance when trained on the often noisy output produced by documentary and descriptive projects. Rather than the curated published data that NLP systems are normally trained on, this research will leverage interlinearized glossed texts from documentary and descriptive field work. Since this research will be performed on ``real live” field data---sometimes the only annotated data available for a language---it may uncover as-yet unforeseen challenges for NLP systems in low-resource settings.
%Specifically, it will use neural networks to learn inflectional patterns in five languages.
%\mans{``neural machine learning'' is perhaps a little too loose: neural networks might be better.}

%Third, it will establish not only that new computational methods can be successfully integrated into language documentation and description, but it will show how linguistics field methods may impact NLP systems and test whether certain NLP assumptions about data annotation should impact linguistics field methods. It will do this by studying what happens when NLP priorities are borrowed for documentary and descriptive linguistics, specifically the priority of part of speech (POS) tagging. Additionally, the discussions of other studies will focus on the characteristics and quality of the manually annotated data and how these characteristics affect the machine learning's performance.

%Fourth, the proposed research will increase annotated data in five low-resource languages. These languages represent a range of linguistic structures and language families. They are spoken by communities across five continents. Interlinear texts in these languages are currently limited to 5-100K words and the amount of descriptive publications is quite low. Increased annotated data will allow more thorough testing of linguistic theories and computational models.
%, which contribute to our understanding of human language and the performance of machine learning algorithms in low-resource settings. 

%If documentary and descriptive IGT that has been manually annotated can be utilized for computational induction of morphological inflection patterns with minimal need for domain experts, then both linguists and NLP practitioners can confidently increase their collaboration. They can further develop NLP for low-resource settings and perhaps focus together on systems that mutually benefits the discipline of linguistics, the development of NLP, and the practical needs of communities engaged in language revitalization or maintenance. 

%If the presence of POS tags makes a significant difference in the results of other automated tasks that are helpful to the documentary and descriptive workflow, then linguists may want to re-examine their accepted workflow and emphasize early analysis and labeling of syntactic categories. If the difference is trivial, then linguists have little reason to allocate early time and funding to POS tagging despite its importance to NLP. NLP practitioners, in turn, may need to reconsider their reliance on POS tags for some tasks. 

