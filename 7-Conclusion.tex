\chapter{Conclusion}
\label{chap:conclusion}

The studies proposed here will investigate the potential to advance the science of linguistics through the integration of NLP machine learning systems. This research has broad intellectual merit and potential impacts. It will provide more usable data in linguistics, leverage field data for NLP models, and test the models efficacy with limited and noisy data which is often the only data available for endangered languages. It also advances murky issues surrounding the nature and role of syntactic categories with an empirical study.

The outcomes of this research will have broader impact on future direction of language documentation and description methods and workflow. If theory-based segmentation choices negatively affect the performance of machine learning models on morpheme segmentation and glossing, then linguists who wish to benefit from automated assistance in this task will have to adjust their choices and their investment in manual annotation consistency. If the annotation quality as well as the amount of data determines how different machine learning models perform, then linguists may need to educate themselves on the difference between machine learning models. If these issues make little difference, then documentary and descriptive linguists can continue with currently accepted methods and workflows.

If documentary and descriptive IGT that has been manually annotated can be utilized for computational induction of morphological inflection patterns with minimal need for domain experts, then both linguists and NLP practitioners can confidently increase their collaboration. They can further develop NLP for low-resource settings and perhaps focus together on systems that mutually benefits the discipline of linguistics, the development of NLP, and the practical needs of communities engaged in language revitalization or maintenance. 

If the presence of POS tags makes a significant difference in the results of other automated tasks that are helpful to the documentary and descriptive workflow, then linguists may want to re-examine their accepted workflow and emphasize early analysis and labeling of syntactic categories. If the difference is trivial, then linguists have little reason to allocate early time and funding to POS tagging despite its importance to NLP. NLP practitioners, in turn, may need to reconsider their reliance on POS tags for some tasks. 

%%%%%%%FUTURE%%%%%%

Human-in-the-loop. 

Segmentation \& Glossing. Towards software interface that works for multiple languages. After initial human annotation. Train model as shown here. Able to take any strategy and get best results. Then predict unannotated data. It should then strategically choose small batches (~50) of tokens. Present context to annotator and have them correct or vet them. Then retrain. The question is how to select those batches so that the model achieves highest accuracy in shortest time. 

Three common active learning strategies are random, uncertainty, and sequential. Compare selecting with linguistic factors and/or selecting with weighting by lingustic factors. General linguistic factors: length of word, similar substrings on words with high uncertainty (assume these indicate morphological forms not yet learned well), POS that have multiple glosses (assumed to indicate morphological complex POS), ...
Language- or areal- or genetic-specific factors: POS that is known to be most complex (e.g. verbs in Upper Tanana), position of word in sentence, specific substring (indicating inflectional class or derivational complications), ...

Compare these strategies using pre-annotated data as stand-in for human annotations. Run one time to see immediate improvement. Run multiple time to see improvement over several iterations. Compare single strategies. Then combine different strategies and compare. Try adding strategies at different points in iterative progress to identify if a strategy is more helpful at certain point. 

Next run with human annotators to see how the strategies interact. Measure both time and accuracy. Measure workhours to achieve preset accuracy goal. Measure average time per annotation and how this changes through iterations. Consider linguistic expertise/familiarity of annotators. 

Build software. Collaborate with HCI and UX. 

Test software and previous factors in actual documentary/descriptive field project. Test the whole process.

\#\#\#

A natural future direction is to address the practical implications of integrating machine learning. One implication is the potential for ``active learning'' where machine learning models are iteratively re-trained with a small set of new manual annotations. A key issue is how to select data for the new manual annotations so that the model will improve very quickly with least manual effort. Selection methods based on computational and linguistic motivations need to be tested and compared. For example, active learning for paradigm induction would involve first training on partial tables, then comparing results after retraining on data selected by different methods: 1) selecting word forms from the modelâ€™s predictions that it had least ``confidence'' about, 2) randomly selecting word forms, 3) selecting a random number word forms but making sure they are evenly distributed among tables, or 4) selecting word forms so that each table has the same minimum number of forms per table. This same experiment could be conducted for joint segmentation and glossing. The first two methods would remain the same, but other methods could be 3) selecting a number of least frequent word types, 4) selecting word forms based on statistically rare letter sequence combinations (in attempt to identify rare morphological combinations), or some other method motivated by linguistic facts of the language such as 5) annotating only nouns in Lezgi which are most likely to have irregular allomorphy. Either of these experiments are potential additional or alternative studies to the research that is described in this Prospectus.
